import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Mapping, Callable
import io
import PIL.Image
from autogen_core import Image as AGImage
import asyncio
from autogen_core import (
    CancellationToken,
    DefaultTopicId,
    MessageContext,
    event,
    rpc,
    AgentId,
)
from autogen_core.models import (
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_core.model_context import TokenLimitedChatCompletionContext
from autogen_agentchat.base import Response, TerminationCondition
from autogen_agentchat.messages import (
    BaseChatMessage,
    StopMessage,
    TextMessage,
    MultiModalMessage,
    BaseAgentEvent,
    MessageFactory,
)
from autogen_agentchat.teams._group_chat._events import (
    GroupChatAgentResponse,
    GroupChatMessage,
    GroupChatRequestPublish,
    GroupChatStart,
    GroupChatTermination,
)
from autogen_agentchat.teams._group_chat._base_group_chat_manager import (
    BaseGroupChatManager,
)
from autogen_agentchat.state import BaseGroupChatManagerState
from ...learning.memory_provider import MemoryControllerProvider

from ...types import HumanInputFormat, Plan
from ...utils import dict_to_str, thread_to_context
from ...tools.bing_search import get_bing_search_results
from ...teams.orchestrator.orchestrator_config import OrchestratorConfig
from ._prompts import (
    get_orchestrator_system_message_planning,
    get_orchestrator_system_message_planning_autonomous,
    get_orchestrator_plan_prompt_json,
    get_orchestrator_plan_replan_json,
    get_orchestrator_progress_ledger_prompt,
    ORCHESTRATOR_SYSTEM_MESSAGE_EXECUTION,
    ORCHESTRATOR_FINAL_ANSWER_PROMPT,
    ORCHESTRATOR_TASK_LEDGER_FULL_FORMAT,
    INSTRUCTION_AGENT_FORMAT,
    validate_ledger_json,
    validate_plan_json,
)
from ._utils import is_accepted_str, extract_json_from_string
from loguru import logger as trace_logger
import time


class OrchestratorState(BaseGroupChatManagerState):
    """
    ğŸ”§ å¢å¼ºçš„OrchestratorçŠ¶æ€ç®¡ç† - æ”¯æŒæ™ºèƒ½ä»»åŠ¡æ‰§è¡Œå’Œä¸Šä¸‹æ–‡ä¼ é€’
    """

    task: str = ""
    plan_str: str = ""
    plan: Plan | None = None
    n_rounds: int = 0
    current_step_idx: int = 0
    information_collected: str = ""
    in_planning_mode: bool = True
    is_paused: bool = False
    group_topic_type: str = ""
    message_history: List[BaseChatMessage | BaseAgentEvent] = []
    participant_topic_types: List[str] = []
    n_replans: int = 0
    
    # ğŸ”§ å¢å¼ºï¼šæ­¥éª¤çŠ¶æ€ç²¾ç¡®è·Ÿè¸ª
    step_execution_status: Dict[int, str] = {}  # {step_index: "not_started"|"in_progress"|"completed"|"failed"|"skipped"}
    current_step_agent_response_count: int = 0  # å½“å‰æ­¥éª¤ä»£ç†å“åº”æ¬¡æ•°
    step_completion_evidence: Dict[int, List[str]] = {}  # {step_index: [evidence_list]}
    last_agent_task_completion_signal: str = ""  # æœ€åä¸€ä¸ªä»£ç†çš„ä»»åŠ¡å®Œæˆä¿¡å·
    
    # ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†
    global_context: Dict[str, Any] = {}  # å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯
    agent_outputs: Dict[str, Any] = {}  # å„ä»£ç†è¾“å‡ºç¼“å­˜
    task_boundaries: Dict[int, Dict[str, Any]] = {}  # æ¯æ­¥ä»»åŠ¡è¾¹ç•Œè®¾ç½®
    execution_metrics: Dict[str, Any] = {}  # æ‰§è¡Œåº¦é‡æ•°æ®
    
    # ğŸ”§ æ–°å¢ï¼šå¾ªç¯æ£€æµ‹å’Œé˜²æŠ¤
    action_history: List[Dict[str, Any]] = []  # æ“ä½œå†å²è®°å½•
    repetition_count: Dict[str, int] = {}  # é‡å¤æ“ä½œè®¡æ•°
    warning_flags: List[str] = []  # è­¦å‘Šæ ‡å¿—
    
    # ğŸ”§ ä¿®å¤ï¼šæ­¥éª¤æ‰§è¡Œæ§åˆ¶æ ‡è®°
    _should_start_next_step: bool = False  # æ ‡è®°æ˜¯å¦åº”è¯¥å¯åŠ¨ä¸‹ä¸€æ­¥
    
    # ğŸ”§ æ–°å¢ï¼šä»»åŠ¡è¿›åº¦è·Ÿè¸ª
    step_start_time: Dict[int, float] = {}  # æ­¥éª¤å¼€å§‹æ—¶é—´
    step_duration_limit: Dict[int, float] = {}  # æ­¥éª¤æ—¶é—´é™åˆ¶
    information_quality_score: Dict[int, float] = {}  # ä¿¡æ¯è´¨é‡è¯„åˆ†

    def reset(self) -> None:
        self.task = ""
        self.plan_str = ""
        self.plan = None
        self.n_rounds = 0
        self.current_step_idx = 0
        self.information_collected = ""
        self.in_planning_mode = True
        self.message_history = []
        self.is_paused = False
        self.n_replans = 0
        
        # ğŸ”§ é‡ç½®å¢å¼ºçŠ¶æ€ç®¡ç†
        self.step_execution_status = {}
        self.current_step_agent_response_count = 0
        self.step_completion_evidence = {}
        self.last_agent_task_completion_signal = ""
        self.global_context = {}
        self.agent_outputs = {}
        self.task_boundaries = {}
        self.execution_metrics = {}
        self.action_history = []
        self.repetition_count = {}
        self.warning_flags = []
        self.step_start_time = {}
        self.step_duration_limit = {}
        self.information_quality_score = {}
        # ğŸ”§ é‡ç½®æ­¥éª¤çŠ¶æ€è·Ÿè¸ª
        self.step_execution_status = {}
        self.current_step_agent_response_count = 0
        self.step_completion_evidence = {}
        self.last_agent_task_completion_signal = ""

    def reset_for_followup(self) -> None:
        self.task = ""
        self.plan_str = ""
        self.plan = None
        self.n_rounds = 0
        self.current_step_idx = 0
        self.in_planning_mode = True
        self.is_paused = False
        self.n_replans = 0
        # ğŸ”§ é‡ç½®æ­¥éª¤çŠ¶æ€è·Ÿè¸ª
        self.step_execution_status = {}
        self.current_step_agent_response_count = 0
        self.step_completion_evidence = {}
        self.last_agent_task_completion_signal = ""


class Orchestrator(BaseGroupChatManager):
    """
    The Orchestrator class is responsible for managing a group chat by orchestrating the conversation
    between multiple participants. It extends the SequentialRoutedAgent class and provides functionality
    to handle the start, reset, and progression of the group chat.

    The orchestrator maintains the state of the conversation, including the task, plan, and progress. It
    interacts with a model client to generate and validate plans, and it publishes messages to the group
    chat based on the current state and responses from participants.

    """

    def __init__(
        self,
        name: str,
        group_topic_type: str,
        output_topic_type: str,
        message_factory: MessageFactory,
        participant_topic_types: List[str],
        participant_descriptions: List[str],
        participant_names: List[str],
        output_message_queue: asyncio.Queue[
            BaseAgentEvent | BaseChatMessage | GroupChatTermination
        ],
        model_client: ChatCompletionClient,
        config: OrchestratorConfig,
        termination_condition: TerminationCondition | None = None,
        max_turns: int | None = None,
        memory_provider: MemoryControllerProvider | None = None,
    ):
        super().__init__(
            name,
            group_topic_type,
            output_topic_type,
            participant_topic_types,
            participant_names,
            participant_descriptions,
            output_message_queue,
            termination_condition,
            max_turns,
            message_factory=message_factory,
        )
        self._model_client: ChatCompletionClient = model_client
        self._model_context = TokenLimitedChatCompletionContext(
            model_client, token_limit=config.model_context_token_limit
        )
        self._config: OrchestratorConfig = config
        self._user_agent_topic = "user_proxy"
        self._web_agent_topic = "web_surfer"
        if self._user_agent_topic not in self._participant_names:
            if not (
                self._config.autonomous_execution
                and not self._config.allow_follow_up_input
            ):
                raise ValueError(
                    f"User agent topic {self._user_agent_topic} not in participant names {self._participant_names}"
                )

        self._memory_controller = None
        self._memory_provider = memory_provider
        if (
            self._config.memory_controller_key
            and self._model_client
            and self._memory_provider is not None
        ):
            try:
                provider = self._memory_provider
                self._memory_controller = provider.get_memory_controller(
                    memory_controller_key=self._config.memory_controller_key,
                    client=self._model_client,
                )
                trace_logger.info("Memory controller initialized successfully.")
            except Exception as e:
                trace_logger.warning(f"Failed to initialize memory controller: {e}")

        # Setup internal variables
        self._setup_internals()

    def _setup_internals(self) -> None:
        """
        Setup internal variables used in orchestrator
        """
        self._state: OrchestratorState = OrchestratorState()

        # Create filtered lists for execution that may exclude the user agent
        self._agent_execution_names = self._participant_names.copy()
        self._agent_execution_descriptions = self._participant_descriptions.copy()

        if self._config.autonomous_execution:
            # Filter out the user agent from execution lists
            user_indices = [
                i
                for i, name in enumerate(self._agent_execution_names)
                if name == self._user_agent_topic
            ]
            if user_indices:
                user_index = user_indices[0]
                self._agent_execution_names.pop(user_index)
                self._agent_execution_descriptions.pop(user_index)
        # add a a new participant for the orchestrator to do nothing
        self._agent_execution_names.append("no_action_agent")
        self._agent_execution_descriptions.append(
            "If for this step no action is needed, you can use this agent to perform no action"
        )

        self._team_description: str = "\n".join(
            [
                f"{topic_type}: {description}".strip()
                for topic_type, description in zip(
                    self._agent_execution_names,
                    self._agent_execution_descriptions,
                    strict=True,
                )
            ]
        )
        self._last_browser_metadata_hash = ""

    def _get_system_message_planning(
        self,
    ) -> str:
        date_today = datetime.now().strftime("%Y-%m-%d")
        if self._config.autonomous_execution:
            return get_orchestrator_system_message_planning_autonomous(
                self._config.sentinel_tasks
            ).format(
                date_today=date_today,
                team=self._team_description,
            )
        else:
            return get_orchestrator_system_message_planning(
                self._config.sentinel_tasks
            ).format(
                date_today=date_today,
                team=self._team_description,
            )

    def _get_task_ledger_plan_prompt(self, team: str) -> str:
        additional_instructions = ""
        if self._config.allowed_websites is not None:
            additional_instructions = (
                "Only use the following websites if possible: "
                + ", ".join(self._config.allowed_websites)
            )

        return get_orchestrator_plan_prompt_json(self._config.sentinel_tasks).format(
            team=team, additional_instructions=additional_instructions
        )

    def _get_task_ledger_replan_plan_prompt(
        self, task: str, team: str, plan: str
    ) -> str:
        additional_instructions = ""
        if self._config.allowed_websites is not None:
            additional_instructions = (
                "Only use the following websites if possible: "
                + ", ".join(self._config.allowed_websites)
            )
        return get_orchestrator_plan_replan_json(self._config.sentinel_tasks).format(
            task=task,
            team=team,
            plan=plan,
            additional_instructions=additional_instructions,
        )

    def _get_task_ledger_full_prompt(self, task: str, team: str, plan: str) -> str:
        return ORCHESTRATOR_TASK_LEDGER_FULL_FORMAT.format(
            task=task, team=team, plan=plan
        )

    def _get_progress_ledger_prompt(
        self, task: str, plan: str, step_index: int, team: str, names: List[str]
    ) -> str:
        assert self._state.plan is not None
        additional_instructions = ""
        if self._config.autonomous_execution:
            additional_instructions = "VERY IMPORTANT: The next agent name cannot be the user or user_proxy, use any other agent."
        
        # ğŸ”§ å¢å¼ºï¼šæ·»åŠ ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¿¡æ¯å’Œè¾¹ç•Œè­¦å‘Šï¼ˆä¿ç•™è‡ªå®šä¹‰åŠŸèƒ½ï¼‰
        enhanced_instructions = additional_instructions
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„ä¸Šä¸‹æ–‡ç”ŸæˆåŠŸèƒ½
        if hasattr(self, '_generate_context_summary') and hasattr(self._state, 'task_boundaries'):
            context_summary = self._generate_context_summary(step_index)
            boundaries = self._state.task_boundaries.get(step_index, {})
            
            if boundaries:
                boundary_info = "\n\nğŸ”§ **å½“å‰æ­¥éª¤è¾¹ç•Œé™åˆ¶**:\n"
                boundary_info += f"- æœ€å¤§æ“ä½œæ•°: {boundaries.get('max_actions', 5)}\n"
                boundary_info += f"- æ—¶é—´é™åˆ¶: {boundaries.get('time_limit', 300)}ç§’\n"
                boundary_info += f"- å½“å‰æ“ä½œæ•°: {self._state.current_step_agent_response_count}\n"
                if boundaries.get('success_criteria'):
                    boundary_info += f"- æˆåŠŸæ ‡å‡†: {', '.join(boundaries['success_criteria'])}\n"
                enhanced_instructions += boundary_info
            
            enhanced_instructions += f"\n\nğŸ”§ **æ‰§è¡Œä¸Šä¸‹æ–‡**:\n{context_summary}"
        
        # ä½¿ç”¨å®˜æ–¹æ›´æ–°çš„å‡½æ•°ï¼ˆåŒ…å«sentinel_tasksæ”¯æŒï¼‰
        return get_orchestrator_progress_ledger_prompt(
            self._config.sentinel_tasks
        ).format(
            task=task,
            plan=plan,
            step_index=step_index,
            step_title=self._state.plan[step_index].title,
            step_details=self._state.plan[step_index].details,
            agent_name=self._state.plan[step_index].agent_name,
            team=team,
            names=", ".join(names),
            additional_instructions=enhanced_instructions,
        )

    def _generate_context_summary(self, current_step_idx: int) -> str:
        """
        ğŸ”§ ç”Ÿæˆå½“å‰æ‰§è¡Œä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œä¸ºAgentæä¾›å…¨å±€çŠ¶æ€ä¿¡æ¯
        """
        context = self._state.global_context
        summary_parts: List[str] = []
        
        # ğŸ”§ è®¡åˆ’è¿›åº¦æ¦‚è§ˆ
        total_steps = len(self._state.plan) if self._state.plan else 0
        completed_count = sum(1 for status in self._state.step_execution_status.values() if status == "completed")
        summary_parts.append(f"ğŸ“‹ è®¡åˆ’è¿›åº¦: {completed_count}/{total_steps} æ­¥éª¤å·²å®Œæˆ")
        
        # ğŸ”§ ç½‘é¡µç ”ç©¶çŠ¶æ€
        if context.get("website_research_completed"):
            product_info = self._extract_product_info(context.get("website_content", ""))
            if product_info.get("camera_found"):
                specs = ", ".join(product_info.get("specifications", []))
                summary_parts.append(f"âœ… ç½‘é¡µç ”ç©¶å·²å®Œæˆ: æ‰¾åˆ°360ç›¸æœºäº§å“ä¿¡æ¯ ({specs})")
            else:
                summary_parts.append("âš ï¸ ç½‘é¡µç ”ç©¶å·²å°è¯•ï¼Œä½†äº§å“ä¿¡æ¯æœ‰é™")
        
        # ğŸ”§ å›¾åƒç”ŸæˆçŠ¶æ€
        if context.get("image_generated"):
            summary_parts.append("âœ… å›¾åƒç”Ÿæˆå·²å®Œæˆï¼Œå¯ç”¨äºæ–‡æ¡£é›†æˆ")
        
        # ğŸ”§ æ–‡æ¡£åˆ›å»ºçŠ¶æ€
        doc_status: List[str] = []
        if context.get("markdown_created"):
            doc_status.append("Markdown")
        if context.get("html_created"):
            doc_status.append("HTML") 
        if context.get("pdf_created"):
            doc_status.append("PDF")
        if doc_status:
            summary_parts.append(f"âœ… æ–‡æ¡£åˆ›å»ºè¿›åº¦: {' â†’ '.join(doc_status)}")
        
        # ğŸ”§ å½“å‰æ­¥éª¤çŠ¶æ€
        boundaries = self._state.task_boundaries.get(current_step_idx, {})
        if boundaries:
            response_count = self._state.current_step_agent_response_count
            max_actions = boundaries.get("max_actions", 5)
            summary_parts.append(f"ğŸ“Š å½“å‰æ­¥éª¤çŠ¶æ€: {response_count}/{max_actions} æ“ä½œ")
        
        # ğŸ”§ è­¦å‘Šä¿¡æ¯
        warnings: List[str] = []
        if self._state.current_step_agent_response_count > 3:
            warnings.append("âš ï¸ å½“å‰æ­¥éª¤æ“ä½œæ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®å°½å¿«å®Œæˆ")
        
        repetition_count = sum(self._state.repetition_count.values())
        if repetition_count > 0:
            warnings.append(f"ğŸ”„ æ£€æµ‹åˆ° {repetition_count} æ¬¡é‡å¤æ“ä½œæ¨¡å¼")
        
        if warnings:
            summary_parts.extend(warnings)
        
        return "\n".join(summary_parts) if summary_parts else "ğŸ“ æ‰§è¡Œå¼€å§‹ï¼Œæš‚æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"

    def _get_final_answer_prompt(self, task: str) -> str:
        if self._config.final_answer_prompt is not None:
            return self._config.final_answer_prompt.format(task=task)
        else:
            return ORCHESTRATOR_FINAL_ANSWER_PROMPT.format(task=task)

    def get_agent_instruction(self, instruction: str, agent_name: str) -> str:
        assert self._state.plan is not None
        
        # ğŸ”§ å¢å¼ºï¼šä¸ºæŒ‡ä»¤æ·»åŠ è‡ªä¸»æ‰§è¡Œä¸Šä¸‹æ–‡
        enhanced_instruction = self._enhance_instruction_with_autonomous_context(
            instruction, agent_name, self._state.current_step_idx
        )
        
        return INSTRUCTION_AGENT_FORMAT.format(
            step_index=self._state.current_step_idx + 1,
            step_title=self._state.plan[self._state.current_step_idx].title,
            step_details=self._state.plan[self._state.current_step_idx].details,
            agent_name=agent_name,
            instruction=enhanced_instruction,
        )

    def _validate_ledger_json(self, json_response: Dict[str, Any]) -> bool:
        return validate_ledger_json(json_response, self._agent_execution_names)

    def _validate_plan_json(self, json_response: Dict[str, Any]) -> bool:
        return validate_plan_json(json_response, self._config.sentinel_tasks)

    # ğŸ”§ æ–°å¢ï¼šæ­¥éª¤çŠ¶æ€ç®¡ç†è¾…åŠ©æ–¹æ³•
    def _init_step_status(self, step_idx: int) -> None:
        """åˆå§‹åŒ–æ­¥éª¤çŠ¶æ€"""
        if step_idx not in self._state.step_execution_status:
            self._state.step_execution_status[step_idx] = "not_started"
            self._state.step_completion_evidence[step_idx] = []

    def _mark_step_in_progress(self, step_idx: int) -> None:
        """ğŸ”§ å¢å¼ºçš„æ­¥éª¤è¿›è¡Œä¸­æ ‡è®° - åŒ…å«è¾¹ç•Œè®¾ç½®å’Œæ—¶é—´è·Ÿè¸ª"""
        import time
        
        self._init_step_status(step_idx)
        self._state.step_execution_status[step_idx] = "in_progress"
        self._state.current_step_agent_response_count = 0
        self._state.step_start_time[step_idx] = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        # ğŸ”§ è®¾ç½®ä»»åŠ¡è¾¹ç•Œ
        if self._state.plan and step_idx < len(self._state.plan):
            step = self._state.plan[step_idx]
            agent_name = getattr(step, 'agent_name', 'unknown')
            boundaries = self._setup_task_boundaries(step_idx, step.title, agent_name)
            trace_logger.info(f"ğŸš€ æ­¥éª¤ {step_idx + 1} å¼€å§‹æ‰§è¡Œ - é™åˆ¶: {boundaries['max_actions']}æ“ä½œ, {boundaries['time_limit']}ç§’")
        else:
            trace_logger.info(f"ğŸš€ æ­¥éª¤ {step_idx + 1} å¼€å§‹æ‰§è¡Œ")

    def _mark_step_completed(self, step_idx: int, evidence: str, completion_type: str = "normal") -> None:
        """ğŸ”§ å¢å¼ºçš„æ­¥éª¤å®Œæˆæ ‡è®° - æ”¯æŒå¤šç§å®Œæˆç±»å‹å’Œè´¨é‡è¯„ä¼°"""
        import time
        
        self._init_step_status(step_idx)
        self._state.step_execution_status[step_idx] = "completed"
        self._state.step_completion_evidence[step_idx].append(evidence)
        
        # ğŸ”§ è®¡ç®—æ‰§è¡Œæ—¶é—´å’Œè´¨é‡è¯„åˆ†
        start_time = self._state.step_start_time.get(step_idx, time.time())
        duration = time.time() - start_time
        
        # ğŸ”§ åŸºäºå®Œæˆç±»å‹å’Œå†…å®¹è´¨é‡è®¡ç®—è¯„åˆ†
        quality_score = self._calculate_step_quality(step_idx, evidence, completion_type, duration)
        self._state.information_quality_score[step_idx] = quality_score
        
        # ğŸ”§ è®°å½•ä¸åŒç±»å‹çš„å®Œæˆ
        completion_messages = {
            "normal": f"âœ… æ­¥éª¤ {step_idx + 1} æ­£å¸¸å®Œæˆ",
            "boundary": f"â° æ­¥éª¤ {step_idx + 1} è¾¾åˆ°è¾¹ç•Œé™åˆ¶å®Œæˆ",
            "forced": f"ğŸ”„ æ­¥éª¤ {step_idx + 1} å¼ºåˆ¶å®Œæˆï¼ˆé¿å…å¾ªç¯ï¼‰",
            "timeout": f"â±ï¸ æ­¥éª¤ {step_idx + 1} è¶…æ—¶å®Œæˆ"
        }
        
        message = completion_messages.get(completion_type, completion_messages["normal"])
        trace_logger.info(f"{message} - è´¨é‡è¯„åˆ†: {quality_score:.2f}, è€—æ—¶: {duration:.1f}ç§’")

    def _calculate_step_quality(self, step_idx: int, evidence: str, completion_type: str, duration: float) -> float:
        """ğŸ”§ è®¡ç®—æ­¥éª¤å®Œæˆè´¨é‡è¯„åˆ†"""
        base_score = 1.0
        
        # ğŸ”§ æ ¹æ®å®Œæˆç±»å‹è°ƒæ•´åŸºç¡€åˆ†æ•°
        type_multipliers = {
            "normal": 1.0,
            "boundary": 0.8,
            "forced": 0.6,
            "timeout": 0.5
        }
        score = base_score * type_multipliers.get(completion_type, 0.5)
        
        # ğŸ”§ æ ¹æ®è¯æ®å†…å®¹è´¨é‡è°ƒæ•´
        evidence_lower = evidence.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®æ­¥éª¤å®Œæˆä¿¡å·ï¼ˆä¿®å¤ï¼šç§»é™¤å…¨å±€ä»»åŠ¡ä¿¡å·ï¼‰
        if any(signal in evidence_lower for signal in ["âœ…", "å½“å‰æ­¥éª¤å·²å®Œæˆ", "step completed", "æ­¥éª¤å®Œæˆ"]):
            score += 0.3
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®è´¨æ€§å†…å®¹
        content_indicators = ["360", "camera", "image", "document", "file", "created", "generated"]
        content_count = sum(1 for indicator in content_indicators if indicator in evidence_lower)
        score += min(content_count * 0.1, 0.4)
        
        # ğŸ”§ æ ¹æ®æ‰§è¡Œæ—¶é—´è°ƒæ•´ï¼ˆé€‚ä¸­çš„æ—¶é—´è·å¾—æ›´é«˜åˆ†æ•°ï¼‰
        boundaries = self._state.task_boundaries.get(step_idx, {})
        time_limit = boundaries.get("time_limit", 300)
        if duration < time_limit * 0.3:  # å¤ªå¿«å¯èƒ½ä¿¡æ¯ä¸è¶³
            score *= 0.9
        elif duration > time_limit * 0.8:  # å¤ªæ…¢æ•ˆç‡ä½
            score *= 0.8
        
        return min(max(score, 0.0), 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…

    def _is_step_truly_complete(self, step_idx: int, agent_response: str) -> bool:
        """
        æ™ºèƒ½æ­¥éª¤å®Œæˆåˆ¤æ–­ - å¤šå±‚æ¬¡åˆ†æç¡®ä¿å‡†ç¡®æ€§å’ŒæŒç»­æ¨è¿›
        """
        if self._state.plan is None or step_idx >= len(self._state.plan):
            return False
            
        step_info = {
            "index": step_idx,
            "title": self._state.plan[step_idx].title.lower(),
            "agent": self._state.plan[step_idx].agent_name,
            "attempts": self._state.current_step_agent_response_count
        }
        
        # æ‰§è¡Œæ™ºèƒ½å®Œæˆæ£€æµ‹
        completion_analysis = self._intelligent_completion_analysis(agent_response, step_info)
        
        trace_logger.info(f"ğŸ§  æ™ºèƒ½å®Œæˆåˆ†æ - æ­¥éª¤{step_idx+1}: ç½®ä¿¡åº¦={completion_analysis['confidence']:.2f}, ç­–ç•¥={completion_analysis['strategy']}")
        
        return completion_analysis["is_complete"]
    
    def _intelligent_completion_analysis(self, agent_response: str, step_info: dict) -> dict:
        """
        æ™ºèƒ½å®Œæˆåˆ†æ - ç¡®ä¿æ€»èƒ½æ‰¾åˆ°æ¨è¿›æ–¹æ¡ˆ
        """
        agent_response_lower = agent_response.lower()
        
        # 1. æ˜ç¡®å®Œæˆä¿¡å·æ£€æµ‹ (æœ€é«˜ä¼˜å…ˆçº§ - 95%ç½®ä¿¡åº¦)
        explicit_signals = [
            "âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ", "å½“å‰æ­¥éª¤å·²å®Œæˆ", "step completed",
            "å·²æˆåŠŸè®¿é—®te720.comå…¨æ™¯ç›¸æœºå®˜ç½‘", "é¿å…è¿›ä¸€æ­¥çš„é‡å¤æµè§ˆ",
            "å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„äº§å“ä¿¡æ¯ç”¨äºåç»­å›¾åƒç”Ÿæˆ", "å·²æ‰§è¡Œå¿…è¦çš„æ“ä½œå¹¶æ”¶é›†åˆ°ç›¸å…³ä¿¡æ¯",
            "å›¾åƒç”Ÿæˆä»»åŠ¡å·²å®Œæˆ", "å›¾åƒå·²æˆåŠŸç”Ÿæˆ", "æ–‡æ¡£åˆ›å»ºä»»åŠ¡å·²å®Œæˆ"
        ]
        
        for signal in explicit_signals:
            if signal in agent_response:
                return {"is_complete": True, "confidence": 0.95, "strategy": "explicit_signal", "evidence": signal}
        
        # 2. ä¸¥æ ¼æœªå®Œæˆæ£€æµ‹ (ç¡®å®šæœªå®Œæˆ - 0%ç½®ä¿¡åº¦)
        definite_incomplete = [
            "æˆ‘ç†è§£æ‚¨éœ€è¦", "ä¸ºäº†åˆ›å»º", "æˆ‘éœ€è¦", "è¯·é—®", "è¿˜éœ€è¦", "è®©æˆ‘", "è®©æˆ‘ä¸ºæ‚¨",
            "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨", "æˆ‘å°†å¸®åŠ©æ‚¨", "è¯·æä¾›æ›´å¤šä¿¡æ¯", "éœ€è¦ä¸€äº›è¯¦ç»†ä¿¡æ¯",
            "i understand", "i can help you", "i will help you", "let me help you"
        ]
        
        if any(pattern in agent_response_lower for pattern in definite_incomplete):
            return {"is_complete": False, "confidence": 0.0, "strategy": "definite_incomplete", "evidence": "generic_help_response"}
        
        # 3. WebSurferç‰¹æ®Šå®Œæˆæ£€æµ‹ (80%ç½®ä¿¡åº¦)
        if step_info["agent"] == "web_surfer":
            websurfer_completion = self._analyze_websurfer_completion(agent_response, step_info)
            if websurfer_completion["score"] > 0.7:
                return {"is_complete": True, "confidence": 0.8, "strategy": "websurfer_behavior", "evidence": websurfer_completion["evidence"]}
        
        # 4. è¯­ä¹‰å†…å®¹åˆ†æ (70%ç½®ä¿¡åº¦)
        semantic_score = self._analyze_semantic_completion(agent_response, step_info)
        if semantic_score > 0.7:
            return {"is_complete": True, "confidence": 0.7, "strategy": "semantic_analysis", "evidence": "content_analysis"}
        
        # 5. é”™è¯¯æ¢å¤è¯„ä¼° (60%ç½®ä¿¡åº¦)
        if self._is_recoverable_error_completion(agent_response):
            return {"is_complete": True, "confidence": 0.6, "strategy": "error_recovery", "evidence": "recoverable_error"}
        
        # 6. è¾¹ç•Œé€‚åº”æœºåˆ¶ (50%ç½®ä¿¡åº¦)
        if self._should_apply_boundary_completion(step_info):
            return {"is_complete": True, "confidence": 0.5, "strategy": "boundary_adaptation", "evidence": "boundary_limits"}
        
        # 7. åå¤‡æ¨è¿›æœºåˆ¶ (40%ç½®ä¿¡åº¦) - ç¡®ä¿æ°¸ä¸å¡æ­»
        if self._should_force_progression(step_info):
            return {"is_complete": True, "confidence": 0.4, "strategy": "fallback_progression", "evidence": "force_progress"}
        
        return {"is_complete": False, "confidence": 0.0, "strategy": "insufficient_evidence", "evidence": "needs_more_work"}
    
    def _analyze_websurfer_completion(self, response: str, step_info: dict) -> dict:
        """åˆ†æWebSurferçš„å®ŒæˆçŠ¶æ€"""
        score = 0.0
        evidence = []
        
        # æ£€æŸ¥è®¿é—®æˆåŠŸ
        access_indicators = ["successfully accessed", "visited", "clicked", "hovered", "navigated", "è®¿é—®", "ç‚¹å‡»"]
        if any(indicator in response.lower() for indicator in access_indicators):
            score += 0.3
            evidence.append("successful_access")
        
        # æ£€æŸ¥äº§å“ç›¸å…³å†…å®¹
        product_indicators = ["te720", "360", "camera", "å…¨æ™¯", "product", "äº§å“", "teche"]
        if any(indicator in response.lower() for indicator in product_indicators):
            score += 0.3
            evidence.append("product_content")
        
        # æ£€æŸ¥ä¿¡æ¯æ”¶é›†
        info_indicators = ["found", "collected", "gathered", "observed", "æ”¶é›†", "è·å–", "æ‰¾åˆ°"]
        if any(indicator in response.lower() for indicator in info_indicators):
            score += 0.2
            evidence.append("information_collected")
        
        # æ£€æŸ¥æ“ä½œæ¬¡æ•° (å¤šæ¬¡å°è¯•ååº”è¯¥å®Œæˆ)
        if step_info["attempts"] >= 3:
            score += 0.2
            evidence.append("multiple_attempts")
        
        return {"score": score, "evidence": evidence}
    
    def _analyze_semantic_completion(self, response: str, step_info: dict) -> float:
        """è¯­ä¹‰å®Œæˆåˆ†æ"""
        score = 0.0
        
        # æ£€æŸ¥å“åº”é•¿åº¦å’Œè¯¦ç»†ç¨‹åº¦
        if len(response) > 100:
            score += 0.2
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“ä¿¡æ¯
        concrete_indicators = ["specifications", "features", "details", "information", "data", "specs", "è§„æ ¼", "ç‰¹ç‚¹", "ä¿¡æ¯"]
        if any(indicator in response.lower() for indicator in concrete_indicators):
            score += 0.3
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°ä¸‹ä¸€æ­¥æˆ–åç»­å¤„ç†
        next_step_indicators = ["next", "subsequent", "continue", "proceed", "ä¸‹ä¸€æ­¥", "åç»­", "ç»§ç»­"]
        if any(indicator in response.lower() for indicator in next_step_indicators):
            score += 0.2
        
        # æ£€æŸ¥ä»»åŠ¡ç›¸å…³å…³é”®è¯
        if step_info["title"]:
            title_words = step_info["title"].split()
            matching_words = sum(1 for word in title_words if word in response.lower())
            score += min(0.3, matching_words * 0.1)
        
        return score
    
    def _is_recoverable_error_completion(self, response: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¯æ¢å¤çš„é”™è¯¯å®Œæˆ"""
        error_indicators = ["error", "timeout", "failed", "exception", "é”™è¯¯", "è¶…æ—¶", "å¤±è´¥"]
        recovery_indicators = ["however", "but", "still", "managed", "successfully", "ä½†æ˜¯", "ä»ç„¶", "æˆåŠŸ"]
        
        has_error = any(indicator in response.lower() for indicator in error_indicators)
        has_recovery = any(indicator in response.lower() for indicator in recovery_indicators)
        
        return has_error and has_recovery
    
    def _should_apply_boundary_completion(self, step_info: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åº”ç”¨è¾¹ç•Œå®Œæˆ"""
        # å¦‚æœå°è¯•æ¬¡æ•°è¿‡å¤šï¼Œåº”è¯¥å®Œæˆ
        if step_info["attempts"] >= 5:
            return True
        
        # å¦‚æœæ˜¯ç ”ç©¶ç±»ä»»åŠ¡ä¸”å·²ç»æœ‰ä¸€å®šå°è¯•
        research_agents = ["web_surfer", "file_surfer"]
        if step_info["agent"] in research_agents and step_info["attempts"] >= 3:
            return True
        
        return False
    
    def _should_force_progression(self, step_info: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼ºåˆ¶æ¨è¿› - ç¡®ä¿æ°¸ä¸å¡æ­»"""
        # ä»»ä½•æ­¥éª¤è¶…è¿‡10æ¬¡å°è¯•éƒ½åº”è¯¥å¼ºåˆ¶æ¨è¿›
        if step_info["attempts"] >= 10:
            trace_logger.warning(f"ğŸ”„ æ­¥éª¤{step_info['index']+1}è¶…è¿‡10æ¬¡å°è¯•ï¼Œå¼ºåˆ¶æ¨è¿›")
            return True
        
        return False

    def _check_websurfer_auto_completion_signals(self, agent_response: str) -> bool:
        """ğŸ”§ æ£€æŸ¥WebSurferçš„è‡ªåŠ¨å®Œæˆä¿¡å·ï¼ˆä»æˆ‘ä»¬çš„ä¿®å¤ä¸­æ·»åŠ ï¼‰"""
        auto_completion_signals = [
            # ä»WebSurferä¿®å¤ä¸­æ·»åŠ çš„å®Œæˆä¿¡å·
            "âœ… å½“å‰æ­¥éª¤å·²å®Œæˆï¼šå·²æˆåŠŸè®¿é—®te720.comå…¨æ™¯ç›¸æœºå®˜ç½‘",
            "è™½ç„¶æ£€æµ‹åˆ°é‡å¤æ“ä½œï¼Œä½†å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„äº§å“ä¿¡æ¯ç”¨äºåç»­å›¾åƒç”Ÿæˆ",
            "é¿å…è¿›ä¸€æ­¥çš„é‡å¤æµè§ˆä»¥æé«˜æ•ˆç‡",
            "å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„äº§å“ä¿¡æ¯ç”¨äºåç»­å›¾åƒç”Ÿæˆ",
            "å·²æˆåŠŸè®¿é—®te720.comå…¨æ™¯ç›¸æœºå®˜ç½‘",
            "å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºåç»­å¤„ç†",
            "å·²æ‰§è¡Œå¿…è¦çš„æ“ä½œå¹¶æ”¶é›†åˆ°ç›¸å…³ä¿¡æ¯",
            # é€šç”¨å®Œæˆä¿¡å·
            "âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ",
            "å½“å‰æ­¥éª¤å·²å®Œæˆ"
        ]
        
        for signal in auto_completion_signals:
            if signal in agent_response:
                trace_logger.info(f"ğŸ¯ æ£€æµ‹åˆ°WebSurferè‡ªåŠ¨å®Œæˆä¿¡å·: {signal}")
                return True
        
        return False

    def _check_research_task_completion(self, response: str) -> bool:
        """æ£€æŸ¥æœç´¢/ç ”ç©¶ç±»ä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        # ğŸ”§ ä¼˜å…ˆæ£€æŸ¥æ˜ç¡®çš„å®Œæˆä¿¡å·
        explicit_completion_signals = [
            "âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ", "âœ… step completed", "å½“å‰æ­¥éª¤å·²å®Œæˆ", "step completed",
            "âš ï¸ å½“å‰æ­¥éª¤å› é”™è¯¯å®Œæˆ", "âš ï¸ step completed with errors", 
            "ğŸ”„ å½“å‰æ­¥éª¤é€šè¿‡æ›¿ä»£æ–¹æ¡ˆå®Œæˆ", "ğŸ”„ step completed via alternative",
            # ğŸ”§ æ–°å¢ï¼šWebSurferè‡ªåŠ¨å®Œæˆä¿¡å·ï¼ˆä»æˆ‘ä»¬çš„ä¿®å¤ä¸­ï¼‰
            "å·²æˆåŠŸè®¿é—®te720.comå…¨æ™¯ç›¸æœºå®˜ç½‘", "é¿å…è¿›ä¸€æ­¥çš„é‡å¤æµè§ˆä»¥æé«˜æ•ˆç‡",
            "å·²æ”¶é›†åˆ°è¶³å¤Ÿçš„äº§å“ä¿¡æ¯ç”¨äºåç»­å›¾åƒç”Ÿæˆ", "å·²æ‰§è¡Œå¿…è¦çš„æ“ä½œå¹¶æ”¶é›†åˆ°ç›¸å…³ä¿¡æ¯"
            # ğŸ”§ ä¿®å¤ï¼šç§»é™¤å…¨å±€ä»»åŠ¡å®Œæˆä¿¡å·ï¼Œåªä¿ç•™æ­¥éª¤å®Œæˆä¿¡å·ä»¥é¿å…è¿‡æ—©ç»ˆæ­¢
        ]
        
        if any(signal in response for signal in explicit_completion_signals):
            trace_logger.info(f"ğŸ¯ æ£€æµ‹åˆ°æ˜ç¡®å®Œæˆä¿¡å·: {[s for s in explicit_completion_signals if s in response]}")
            return True
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç‰¹åˆ«æ£€æŸ¥te720.comè®¿é—®æˆåŠŸçš„æƒ…å†µ
        te720_success_indicators = [
            "te720.com", "teche", "å…¨æ™¯ç›¸æœº", "360åº¦å…¨æ™¯ç›¸æœº", "5GVRç›´æ’­ç³»ç»Ÿ", 
            "Techeå®˜ç½‘", "panoramic camera", "360 camera", "å…¨æ™¯ç›¸æœº-Techeå®˜ç½‘", 
            "ä¸TikTokåˆä½œ", "3D180VR", "8Kæœºå†…ç›´æ’­", "å¾®å•çº§ç”»è´¨", "VRä¸»æ’­",
            "successfully accessed.*te720", "æˆåŠŸè®¿é—®.*te720"
        ]
        
        # æ£€æŸ¥æ˜¯å¦è®¿é—®äº†te720.comä¸”è·å¾—äº†ç›¸å…³ä¿¡æ¯
        has_te720_access = any(indicator in response for indicator in te720_success_indicators)
        
        if has_te720_access:
            trace_logger.info(f"ğŸ¯ æ£€æµ‹åˆ°te720.comè®¿é—®æˆåŠŸï¼ŒåŒ…å«ç›¸å…³äº§å“ä¿¡æ¯")
            return True
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥WebSurferæ˜¯å¦å·²æˆåŠŸè®¿é—®ç½‘ç«™å¹¶è·å–äº†ä¿¡æ¯
        website_access_indicators = [
            "successfully accessed", "æˆåŠŸè®¿é—®", "è®¿é—®äº†", "hovered over", 
            "visited", "accessed", "loaded", "é¡µé¢", "webpage", "ç½‘ç«™", "website",
            "clicked", "ç‚¹å‡»", "æµè§ˆ", "browse", "navigate", "å¯¼èˆª"
        ]
        
        content_extraction_indicators = [
            "found", "è·å–äº†", "æ”¶é›†äº†", "çœ‹åˆ°äº†", "è§‚å¯Ÿåˆ°", "æ˜¾ç¤º", "åŒ…å«",
            "contains", "shows", "displays", "extracted", "gathered", "discovered",
            "text in the viewport", "é¡µé¢æ˜¾ç¤º", "viewport shows", "positioned at"
        ]
        
        # ğŸ”§ æ£€æŸ¥æ˜¯å¦åŒæ—¶æœ‰è®¿é—®å’Œå†…å®¹æå–çš„è¯æ®
        has_access = any(indicator in response for indicator in website_access_indicators)
        has_content = any(indicator in response for indicator in content_extraction_indicators)
        
        # ğŸ”§ æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“çš„äº§å“ä¿¡æ¯æˆ–æŠ€æœ¯ç»†èŠ‚
        substantive_content_indicators = [
            "360anywhere", "4é•œå¤´", "8k", "å…¨æ™¯ç›¸æœº", "panoramic camera",
            "teche", "te720", "360åº¦", "360 degree", "é•œå¤´åˆ†å¸ƒ", "lens distribution",
            "æŠ€æœ¯è§„æ ¼", "technical specs", "äº§å“ç‰¹ç‚¹", "product features", "äº§å“",
            "360star", "äº†è§£æ›´å¤š", "å…¨æ™¯", "camera", "product", "VR", "ç›´æ’­", "æ‹æ‘„"
        ]
        
        has_substantive_content = any(indicator in response for indicator in substantive_content_indicators)
        
        # ğŸ”§ é‡è¦ä¿®å¤ï¼šå¦‚æœWebSurferå·²ç»è®¿é—®äº†ç½‘ç«™å¹¶è·å¾—äº†ç›¸å…³ä¿¡æ¯ï¼Œå°±è®¤ä¸ºæ­¥éª¤å®Œæˆ
        if has_access and (has_content or has_substantive_content):
            trace_logger.info(f"ğŸ¯ WebSurferç ”ç©¶ä»»åŠ¡å®Œæˆ - è®¿é—®: {has_access}, å†…å®¹: {has_content}, å®è´¨ä¿¡æ¯: {has_substantive_content}")
            return True
        
        # ğŸ”§ é¢å¤–ä¿®å¤ï¼šæ£€æŸ¥WebSurferçš„å…¸å‹è¡Œä¸ºæ¨¡å¼
        websurfer_action_patterns = [
            "hovered over", "clicked", "visited", "accessed", "navigated",
            "æ‚¬åœ", "ç‚¹å‡»", "è®¿é—®", "å¯¼èˆª", "æµè§ˆ", "action:", "observation:"
        ]
        
        # å¦‚æœåŒ…å«WebSurferè¡Œä¸ºæ¨¡å¼ä¸”æ¶‰åŠäº§å“ç›¸å…³å†…å®¹ï¼Œè®¤ä¸ºå®Œæˆ
        has_websurfer_actions = any(pattern in response for pattern in websurfer_action_patterns)
        
        if has_websurfer_actions and has_substantive_content:
            trace_logger.info(f"ğŸ¯ WebSurferè¡Œä¸ºæ¨¡å¼åŒ¹é…å®Œæˆ - è¡Œä¸º: {has_websurfer_actions}, äº§å“ä¿¡æ¯: {has_substantive_content}")
            return True
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå¤„ç†WebSurferé”™è¯¯æ¢å¤åœºæ™¯
        if "encountered an error" in response or "screenshot" in response:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆåŠŸçš„é¡µé¢è®¿é—®å’Œæ“ä½œ
            error_recovery_indicators = [
                "te720.com", "äº§å“", "product", "clicked", "è®¿é—®", 
                "successfully accessed", "action:", "observation:",
                "å·²æ”¶é›†åˆ°åŸºæœ¬é¡µé¢ä¿¡æ¯", "é¡µé¢å¯¼èˆªæ­£å¸¸", "å·²å®Œæˆçš„æ“ä½œ",
                "å…¨æ™¯ç›¸æœº", "360", "teche", "panoramic", "camera"
            ]
            
            has_error_recovery = any(indicator in response.lower() for indicator in error_recovery_indicators)
            
            if has_error_recovery:
                trace_logger.info(f"ğŸ”„ WebSurferé”™è¯¯æ¢å¤å®Œæˆ - åŒ…å«æˆåŠŸæ“ä½œ: {has_error_recovery}")
                return True
        
        # ğŸ”§ ç½‘ç»œé”™è¯¯çš„è¡¥å¿æªæ–½æ£€æŸ¥
        if "error" in response or "è¿æ¥" in response or "connection" in response:
            alternative_indicators = [
                "åŸºäº", "å‚è€ƒ", "æ ¹æ®ä¸€èˆ¬", "ä½¿ç”¨é»˜è®¤", "åˆ›å»ºåŸºç¡€", "åŸºç¡€ç‰ˆæœ¬",
                "based on", "reference", "using general", "create basic", "basic version",
                "alternative", "instead", "however", "nevertheless"
            ]
            return any(alt in response for alt in alternative_indicators)
        
        # ğŸ”§ æ£€æŸ¥åŸºæœ¬å®ŒæˆæŒ‡æ ‡ï¼Œä½†è¦æ±‚æœ‰å®è´¨å†…å®¹
        basic_completion_indicators = [
            "è®¿é—®äº†", "è·å–äº†", "æ‰¾åˆ°äº†", "æœç´¢åˆ°", "é˜…è¯»äº†", "äº†è§£åˆ°", "æ”¶é›†åˆ°",
            "found", "obtained", "searched", "accessed", "gathered", "visited",
            "å‚è€ƒ", "ä¿¡æ¯", "å†…å®¹", "æ•°æ®", "èµ„æ–™", "æˆåŠŸè®¿é—®", "ç½‘ç«™å†…å®¹",
            "page content", "website information", "successfully accessed"
        ]
        
        has_basic_completion = any(indicator in response for indicator in basic_completion_indicators)
        
        # åªæœ‰åœ¨æœ‰å®è´¨å†…å®¹çš„æƒ…å†µä¸‹æ‰è®¤ä¸ºåŸºæœ¬å®Œæˆæœ‰æ•ˆ
        if has_basic_completion and has_substantive_content:
            trace_logger.info(f"ğŸ¯ åŸºæœ¬å®Œæˆæ£€æŸ¥é€šè¿‡ - åŸºæœ¬å®Œæˆ: {has_basic_completion}, å®è´¨å†…å®¹: {has_substantive_content}")
            return True
        
        # ğŸ”§ ç‰¹æ®Šæƒ…å†µï¼šæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºç½‘é¡µç»“æ„é—®é¢˜å¯¼è‡´çš„è¯¯åˆ¤
        if any(indicator in response for indicator in ["The website was accessible", "successfully accessed", "æˆåŠŸè®¿é—®"]):
            # å¦‚æœæˆåŠŸè®¿é—®äº†ç½‘ç«™ï¼Œå³ä½¿åç»­æœ‰é—®é¢˜ï¼Œä¹Ÿåº”è¯¥è®¤ä¸ºä»»åŠ¡åŸºæœ¬å®Œæˆ
            trace_logger.info(f"ğŸ¯ ç½‘ç«™è®¿é—®æˆåŠŸæ£€æŸ¥é€šè¿‡")
            return True
        
        return False

    def _check_image_generation_completion(self, response: str) -> bool:
        """æ£€æŸ¥å›¾åƒç”Ÿæˆä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        completion_indicators = [
            "å›¾åƒç”Ÿæˆä»»åŠ¡å·²å®Œæˆ", "å›¾åƒå·²æˆåŠŸç”Ÿæˆ", "successfully generated",
            "task_complete", "completed", "å·²å®Œæˆ", "ç”Ÿæˆå®Œæˆ", "å›¾åƒç”Ÿæˆå®Œæˆ",
            "image generation complete", "generation completed", "successfully created",
            "å›¾åƒå·²ç”Ÿæˆ", "ç”Ÿæˆçš„å›¾åƒ", "created image", "generated image"
        ]
        return any(indicator in response for indicator in completion_indicators)

    def _check_document_creation_completion(self, response: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£åˆ›å»ºä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        completion_indicators = [
            "æ–‡ä»¶", "åˆ›å»º", "ä¿å­˜", ".md", "markdown", "äº§å“ä»‹ç»", "æ–‡æ¡£", "æ€»ç»“",
            "å·²å®Œæˆ", "ç”Ÿæˆäº†", "åˆ›å»ºäº†", "å®Œæˆäº†", "å†™å¥½äº†", "å‡†å¤‡å¥½äº†",
            "file created", "document", "completed", "generated", "æ–‡æ¡£åˆ›å»ºä»»åŠ¡å·²å®Œæˆ",
            "summary completed", "introduction created", "document ready", "ä¿å­˜ä¸º",
            "saved as", "æ–‡ä»¶ä¿å­˜", "document saved"
        ]
        return any(indicator in response for indicator in completion_indicators)

    def _check_html_conversion_completion(self, response: str) -> bool:
        """æ£€æŸ¥HTMLè½¬æ¢ä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        completion_indicators = [
            "html", "æ’ç‰ˆ", "è½¬æ¢", "æ ¼å¼åŒ–", "å¸ƒå±€", "æ ·å¼",
            "formatted", "converted", "htmlæ–‡æ¡£åˆ›å»ºä»»åŠ¡å·²å®Œæˆ", "layout completed",
            "htmlæ–‡ä»¶", "ç½‘é¡µ", "é¡µé¢", "htmlæ ¼å¼", "htmlæ–‡æ¡£",
            "html file", "web page", "html format", "html document", "è½¬æ¢å®Œæˆ"
        ]
        return any(indicator in response for indicator in completion_indicators)

    def _check_pdf_generation_completion(self, response: str) -> bool:
        """æ£€æŸ¥PDFç”Ÿæˆä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        completion_indicators = [
            "pdf", "è¾“å‡º", "æœ€ç»ˆ", "å¯¼å‡º", "ç”Ÿæˆ", "è½¬æ¢å®Œæˆ",
            "final", "output", "generated", "pdfæ–‡æ¡£åˆ›å»ºä»»åŠ¡å·²å®Œæˆ",
            "pdfæ–‡ä»¶", "pdfæ ¼å¼", "pdfè½¬æ¢", "final pdf",
            "pdf file", "pdf format", "pdf conversion", "pdf ready", "pdfç”Ÿæˆå®Œæˆ"
        ]
        return any(indicator in response for indicator in completion_indicators)

    def _check_general_task_completion(self, response: str, full_response: str) -> bool:
        """æ£€æŸ¥é€šç”¨ä»»åŠ¡å®ŒæˆçŠ¶æ€"""
        completion_indicators = [
            "å®Œæˆ", "æˆåŠŸ", "å·²", "finished", "completed", "done",
            "successfully", "achieved", "ready", "å‡†å¤‡å¥½", "ç»“æŸ"
        ]
        
        # ğŸ”§ ç¡®ä¿ä¸æ˜¯é€šç”¨å›å¤
        if len(full_response.strip()) < 50:  # å¤ªçŸ­çš„å›å¤é€šå¸¸æ˜¯é€šç”¨å›å¤
            return False
            
        return any(indicator in response for indicator in completion_indicators)

    def _setup_task_boundaries(self, step_idx: int, step_title: str, agent_name: str) -> Dict[str, Any]:
        """
        ğŸ”§ ä¸ºæ¯ä¸ªæ­¥éª¤è®¾ç½®æ™ºèƒ½ä»»åŠ¡è¾¹ç•Œ
        """
        boundaries: Dict[str, Any] = {
            "max_actions": 5,  # é»˜è®¤æœ€å¤§æ“ä½œæ•°
            "time_limit": 300,  # é»˜è®¤5åˆ†é’Ÿæ—¶é—´é™åˆ¶
            "success_criteria": [],
        }
        
        step_lower = step_title.lower()
        
        # ğŸ”§ è®¾ç½®æ­¥éª¤å¼€å§‹æ—¶é—´
        self._state.step_start_time[step_idx] = time.time()
        
        # ğŸ”§ åŸºäºä»»åŠ¡ç±»å‹è®¾ç½®è‡ªä¸»æ‰§è¡Œè¾¹ç•Œ
        if "è®¿é—®" in step_lower or "browse" in step_lower or "find" in step_lower:
            boundaries.update({
                "max_actions": 4,  # æµè§ˆä»»åŠ¡é™åˆ¶4ä¸ªæ“ä½œ
                "time_limit": 180,  # 3åˆ†é’Ÿé™åˆ¶  
                "autonomous_mode": True,  # ğŸ”§ å¯ç”¨è‡ªä¸»æ¨¡å¼
                "approval_threshold": "never",  # ğŸ”§ ç ”ç©¶ä»»åŠ¡ä¸éœ€è¦approval
                "success_criteria": [
                    "æ‰¾åˆ°äº§å“å›¾ç‰‡", "è·å–åŸºæœ¬ä¿¡æ¯", "è®¿é—®æˆåŠŸ"
                ],
                "stop_conditions": [
                    "é‡å¤æ“ä½œè¶…è¿‡2æ¬¡", "å·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯", "è¿æ¥é”™è¯¯ä¸”æœ‰æ›¿ä»£æ–¹æ¡ˆ"
                ],
                "context_requirements": [
                    "äº§å“åç§°", "æŠ€æœ¯è§„æ ¼", "è§†è§‰å‚è€ƒ"
                ]
            })
        elif "é˜…è¯»" in step_lower or "read" in step_lower or "information" in step_lower:
            boundaries.update({
                "max_actions": 3,  # é˜…è¯»ä»»åŠ¡é™åˆ¶3ä¸ªæ“ä½œ
                "time_limit": 120,  # 2åˆ†é’Ÿé™åˆ¶
                "success_criteria": [
                    "è·å–è¯¦ç»†ä»‹ç»", "ç†è§£äº§å“ç‰¹ç‚¹", "æ”¶é›†æŠ€æœ¯ä¿¡æ¯"
                ]
            })
        elif "ç”Ÿæˆ" in step_lower or "generate" in step_lower or "image" in step_lower:
            boundaries.update({
                "max_actions": 1,  # å›¾åƒç”Ÿæˆé€šå¸¸å•æ¬¡æ“ä½œ
                "time_limit": 60,  # 1åˆ†é’Ÿé™åˆ¶
                "success_criteria": [
                    "å›¾åƒç”ŸæˆæˆåŠŸ", "ç¬¦åˆè§„æ ¼è¦æ±‚"
                ]
            })
        elif "åˆ›å»º" in step_lower or "create" in step_lower or "write" in step_lower:
            boundaries.update({
                "max_actions": 2,  # æ–‡æ¡£åˆ›å»ºå¯èƒ½éœ€è¦2ä¸ªæ“ä½œ
                "time_limit": 180,  # 3åˆ†é’Ÿé™åˆ¶
                "success_criteria": [
                    "æ–‡æ¡£åˆ›å»ºå®Œæˆ", "å†…å®¹å®Œæ•´", "æ ¼å¼æ­£ç¡®"
                ]
            })
        
        # ğŸ”§ ä¿å­˜è¾¹ç•Œè®¾ç½®
        self._state.task_boundaries[step_idx] = boundaries
        return boundaries

    def _check_task_boundaries(self, step_idx: int) -> Dict[str, Any]:
        """
        ğŸ”§ æ£€æŸ¥ä»»åŠ¡è¾¹ç•Œæ˜¯å¦è¢«è§¦åŠ
        """
        import time
        
        boundaries = self._state.task_boundaries.get(step_idx, {})
        current_time = time.time()
        start_time = self._state.step_start_time.get(step_idx, current_time)
        
        violations = {
            "max_actions_exceeded": False,
            "time_limit_exceeded": False,
            "repetition_detected": False,
            "should_force_complete": False
        }
        
        # æ£€æŸ¥æ“ä½œæ¬¡æ•°é™åˆ¶
        if self._state.current_step_agent_response_count > boundaries.get("max_actions", 5):
            violations["max_actions_exceeded"] = True
            violations["should_force_complete"] = True
        
        # æ£€æŸ¥æ—¶é—´é™åˆ¶
        if current_time - start_time > boundaries.get("time_limit", 300):
            violations["time_limit_exceeded"] = True
            violations["should_force_complete"] = True
        
        # æ£€æŸ¥é‡å¤æ“ä½œ
        action_key = f"step_{step_idx}_actions"
        if self._state.repetition_count.get(action_key, 0) > 2:
            violations["repetition_detected"] = True
            violations["should_force_complete"] = True
        
        return violations

    def _update_global_context(self, agent_name: str, step_idx: int, content: object) -> None:
        """
        ğŸ”§ æ›´æ–°å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…¼å®¹TextMessageã€MultiModalMessageã€strï¼Œå›¾ç‰‡å¯¹è±¡åªå­˜base64
        """
        from autogen_agentchat.messages import MultiModalMessage, TextMessage
        from autogen_core import Image as AGImage
        image_text = ""
        image_base64 = None
        # å…¼å®¹å¤šæ¨¡æ€
        if hasattr(content, "content") and isinstance(content, MultiModalMessage):
            for part in content.content:
                if isinstance(part, str):
                    image_text += part + "\n"
                elif isinstance(part, AGImage):
                    try:
                        image_base64 = part.to_base64()
                    except Exception:
                        pass
        elif hasattr(content, "content") and isinstance(content, TextMessage):
            image_text = content.content
        elif isinstance(content, str):
            image_text = content
        # åªå­˜æ–‡æœ¬å’Œbase64
        if agent_name == "image_generator" and image_base64:
            self._state.global_context["generated_image_base64"] = image_base64
        self._state.global_context[f"{agent_name}_step_{step_idx}_text"] = image_text
        # å…¼å®¹åŸæœ‰é€»è¾‘
        if agent_name == "web_surfer":
            product_info = self._extract_product_info(image_text or "")
            self._state.global_context.update({
                "product_research": product_info,
                "web_research_completed": True,
                "last_web_content": (image_text or "")[:500]
            })
        elif agent_name == "image_generator":
            self._state.global_context["image_generated"] = True
            if image_text:
                self._state.global_context["generated_image_text"] = image_text
            if image_base64:
                self._state.global_context["image_ready_for_integration"] = True
        elif agent_name == "coder_agent":
            if image_text and ("markdown" in image_text.lower() or ".md" in image_text.lower()):
                self._state.global_context["markdown_created"] = True
            if image_text and "html" in image_text.lower():
                self._state.global_context["html_created"] = True
            if image_text and "pdf" in image_text.lower():
                self._state.global_context["pdf_created"] = True
        self._state.agent_outputs[f"{agent_name}_step_{step_idx}"] = image_text

    def _extract_product_info(self, content: str) -> Dict[str, Any]:
        """
        ğŸ”§ ä»å†…å®¹ä¸­æå–äº§å“ä¿¡æ¯ï¼Œæ”¯æŒ360ç›¸æœºç›¸å…³çš„å…³é”®è¯æ£€æµ‹
        """
        content_lower = content.lower()
        product_info: Dict[str, Any] = {
            "camera_found": False,
            "specifications": [],
            "features": [],
        }
        
        # ğŸ”§ äº§å“æ£€æµ‹
        camera_keywords = ["å…¨æ™¯ç›¸æœº", "360", "panoramic", "camera", "vr", "é•œå¤´", "lens", "teche"]
        if any(keyword in content_lower for keyword in camera_keywords):
            product_info["camera_found"] = True
        
        # æå–æŠ€æœ¯è§„æ ¼
        if "8k" in content_lower:
            product_info["specifications"].append("8Kåˆ†è¾¨ç‡")
        if "4é•œå¤´" in content_lower or "4-lens" in content_lower:
            product_info["specifications"].append("4é•œå¤´é…ç½®")
        if "å®æ—¶" in content_lower or "real-time" in content_lower:
            product_info["features"].append("å®æ—¶å¤„ç†")
        
        return product_info

    def _detect_repetitive_response(self, response: str) -> bool:
        """
        ğŸ”§ æ£€æµ‹é‡å¤å“åº”æ¨¡å¼
        """
        response_lower = response.lower()
        
        # æ£€æŸ¥å¸¸è§çš„é‡å¤æ“ä½œæ¨¡å¼
        repetitive_patterns = [
            "will click.*äº†è§£æ›´å¤š",
            "click.*äº†è§£æ›´å¤š.*click.*äº†è§£æ›´å¤š",
            "i will click.*product.*i will click",
            "è®¿é—®.*è®¿é—®", "æµè§ˆ.*æµè§ˆ",
            "same.*action.*repeatedly"
        ]
        
        import re
        for pattern in repetitive_patterns:
            if re.search(pattern, response_lower):
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„æ“ä½œæè¿°
        if response_lower.count("click") > 2 or response_lower.count("äº†è§£æ›´å¤š") > 1:
            return True
        
        return False

    def _assign_agent_for_task(self, instruction_content: str, step_title: str) -> str:
        """
        ğŸ”§ ç»Ÿä¸€ä»£ç†åˆ†é…é€»è¾‘ - ä¼˜å…ˆçº§é©±åŠ¨çš„æ¸…æ™°åˆ†é…ç­–ç•¥
        """
        instruction_lower = instruction_content.lower()
        step_title_lower = step_title.lower()
        combined_text = (step_title_lower + " " + instruction_lower).strip()
        
        # ğŸ”§ é«˜ä¼˜å…ˆçº§ï¼šç‰¹å®šç»„åˆåŒ¹é…ï¼ˆæœ€å…·ä½“çš„è§„åˆ™ï¼‰
        # å›¾åƒç”Ÿæˆï¼šå¿…é¡»åŒæ—¶åŒ…å«å›¾åƒå…³é”®å­—å’Œä¸»é¢˜å…³é”®å­—
        if (any(kw in combined_text for kw in ["å›¾åƒ", "å›¾ç‰‡", "ç”»", "image", "generate", "create"]) and 
            any(kw in combined_text for kw in ["camera", "ç›¸æœº", "è®¾å¤‡", "äº§å“"])):
            return "image_generator"
        
        # ç½‘ç«™è®¿é—®ï¼šæ˜ç¡®çš„ç½‘ç«™æˆ–æœç´¢ä»»åŠ¡
        if any(kw in combined_text for kw in ["è®¿é—®", "æµè§ˆ", "æœç´¢", "ç½‘ç«™", "te720", "teche720", ".com", "visit", "browse", "search"]):
            return "web_surfer"
        
        # ğŸ”§ ä¸­ä¼˜å…ˆçº§ï¼šæ–‡æ¡£å¤„ç†ï¼ˆæŒ‰å…·ä½“ç¨‹åº¦æ’åºï¼‰
        # PDFè¾“å‡ºï¼šæœ€å…·ä½“çš„æ–‡æ¡£æ ¼å¼
        if (any(kw in combined_text for kw in ["pdf", "è¾“å‡º"]) and 
            any(kw in combined_text for kw in ["æ–‡æ¡£", "document", "generate", "create"])):
            return "coder_agent"
        
        # HTMLæ ¼å¼åŒ–
        if any(kw in combined_text for kw in ["html", "æ’ç‰ˆ", "format", "convert", "styling"]):
            return "coder_agent"
        
        # æ–‡æ¡£åˆ›å»º
        if any(kw in combined_text for kw in ["æ–‡æ¡£", "ä»‹ç»", "markdown", "md", "æ€»ç»“", "æ”¶é›†", "document", "introduction", "summary"]):
            return "coder_agent"
        
        # ğŸ”§ ä½ä¼˜å…ˆçº§ï¼šé€šç”¨ä»»åŠ¡
        # æ–‡ä»¶æ“ä½œ
        if any(kw in combined_text for kw in ["æ–‡ä»¶", "è¯»å–", "æŸ¥çœ‹", "æ‰“å¼€", "file", "read", "open"]):
            return "file_surfer"
        
        # ç¼–ç¨‹ä»»åŠ¡
        if any(kw in combined_text for kw in ["ä»£ç ", "ç¼–ç¨‹", "è„šæœ¬", "è®¡ç®—", "code", "script", "programming"]):
            return "coder_agent"
        
        # ğŸ”§ é»˜è®¤ç­–ç•¥ï¼šæœªåŒ¹é…æ—¶ä½¿ç”¨æ™ºèƒ½é»˜è®¤
        # å¦‚æœåŒ…å«"ç”Ÿæˆ"ä½†æ²¡æœ‰æ˜ç¡®æŒ‡å‘ï¼Œåˆ†é…ç»™coder_agent
        if any(kw in combined_text for kw in ["ç”Ÿæˆ", "åˆ›å»º", "åˆ¶ä½œ", "generate", "create", "make"]):
            return "coder_agent"
        
        # æœ€åé»˜è®¤åˆ†é…ç»™web_surferï¼ˆä¿¡æ¯æ”¶é›†èƒ½åŠ›æœ€å¼ºï¼‰
        return "web_surfer"
    
    def _enhance_instruction_with_autonomous_context(self, instruction: str, agent_name: str, step_idx: int) -> str:
        """
        ğŸ”§ ä¸ºæŒ‡ä»¤æ·»åŠ è‡ªä¸»æ‰§è¡Œä¸Šä¸‹æ–‡å’Œè¾¹ç•Œè®¾ç½®ï¼Œå¹¶è‡ªåŠ¨æ’å›¾è¯´æ˜
        """
        enhanced_instruction = instruction
        # ğŸ”§ ä¸ºweb_surferæ·»åŠ è‡ªä¸»æ‰§è¡ŒæŒ‡å¯¼
        if agent_name == "web_surfer":
            autonomous_guidance = "\n\nğŸ”§ **AUTONOMOUS EXECUTION GUIDANCE**:\n"
            
            boundaries = self._state.task_boundaries.get(step_idx, {})
            if boundaries.get("approval_threshold") == "never":
                autonomous_guidance += "- AUTONOMOUS MODE: Navigate and click freely without approval requests for research purposes.\n"
                autonomous_guidance += f"- ACTION LIMIT: Maximum {boundaries.get('max_actions', 5)} actions for efficiency.\n"
                autonomous_guidance += f"- TIME LIMIT: Complete within {boundaries.get('time_limit', 300)} seconds.\n"
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = self._state.global_context
            if context.get("web_research_completed"):
                autonomous_guidance += "- CONTEXT: Previous web research completed, focus on specific details.\n"
            else:
                autonomous_guidance += "- GOAL: Primary information gathering - focus on key product details.\n"
            
            # æ·»åŠ æˆåŠŸæ ‡å‡†
            success_criteria = boundaries.get("success_criteria", [])
            if success_criteria:
                autonomous_guidance += f"- SUCCESS CRITERIA: {', '.join(success_criteria)}\n"
            
            autonomous_guidance += "- COMPLETION: Use stop_action with âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ when objectives are met.\n"
            enhanced_instruction += autonomous_guidance
        
        # ğŸ”§ ä¸ºimage_generatoræ·»åŠ ä¸Šä¸‹æ–‡å‚è€ƒ
        elif agent_name == "image_generator":
            context = self._state.global_context
            product_info = context.get("product_research", {})
            
            if product_info.get("camera_found"):
                context_guidance = "\n\nğŸ”§ **CONTEXT FROM RESEARCH**:\n"
                specs = product_info.get("specifications", [])
                if specs:
                    context_guidance += f"- Product specifications: {', '.join(specs)}\n"
                
                features = product_info.get("key_features", [])
                if features:
                    context_guidance += f"- Key features: {', '.join(features)}\n"
                
                context_guidance += "- Use this information to create accurate product visualization.\n"
                enhanced_instruction += context_guidance
        
        # ğŸ”§ ä¸ºcoder_agentæ·»åŠ æ–‡æ¡£å·¥ä½œæµæŒ‡å¯¼å’Œè‡ªåŠ¨æ’å›¾è¯´æ˜
        elif agent_name == "coder_agent":
            context = self._state.global_context
            workflow_guidance = "\n\nğŸ”§ **DOCUMENT WORKFLOW GUIDANCE**:\n"
            if context.get("image_generated") and context.get("generated_image_base64"):
                workflow_guidance += "- IMAGE AVAILABLE: è¯·åœ¨æ–‡æ¡£å¼€å¤´æ’å…¥å¦‚ä¸‹å›¾ç‰‡ï¼ˆbase64ï¼‰ï¼š![](data:image/png;base64,{})\n".format(context["generated_image_base64"][:60] + '...')
            if context.get("image_generated"):
                workflow_guidance += "- IMAGE AVAILABLE: Reference generated images in your document.\n"
            if context.get("web_research_completed"):
                workflow_guidance += "- RESEARCH DATA: Use collected web information for content.\n"
            # æ ¹æ®æ­¥éª¤ç±»å‹æ·»åŠ ç‰¹å®šæŒ‡å¯¼
            if "markdown" in instruction.lower():
                workflow_guidance += "- OUTPUT: Create structured .md file for further processing.\n- è¯·ç¡®ä¿äº§å“ä»‹ç»é…å›¾åœ¨æ–‡æ¡£å¼€å¤´ã€‚\n"
            elif "html" in instruction.lower():
                workflow_guidance += "- CONVERSION: Transform markdown to styled HTML with embedded CSS.\n- è¯·å°†ä¸Šä¸€è½®ç”Ÿæˆçš„å›¾ç‰‡åµŒå…¥ HTML æ–‡æ¡£ã€‚\n"
            elif "pdf" in instruction.lower():
                workflow_guidance += "- FINAL OUTPUT: Convert HTML to PDF for distribution.\n- ç¡®ä¿ PDF åŒ…å«äº§å“ä»‹ç»å’Œå›¾ç‰‡ã€‚\n"
            enhanced_instruction += workflow_guidance
        
        return enhanced_instruction

    async def validate_group_state(
        self, messages: List[BaseChatMessage] | None
    ) -> None:
        pass

    async def select_speaker(
        self, thread: List[BaseAgentEvent | BaseChatMessage]
    ) -> str:
        """Not used in this class."""
        return ""

    async def reset(self) -> None:
        """Reset the group chat manager."""
        if self._termination_condition is not None:
            await self._termination_condition.reset()
        self._state.reset()

    async def _log_message(self, log_message: str) -> None:
        trace_logger.debug(log_message)

    async def _log_message_agentchat(
        self,
        content: str,
        internal: bool = False,
        metadata: Optional[Dict[str, str]] = None,
    ) -> None:
        internal_str = "yes" if internal else "no"
        message = TextMessage(
            content=content,
            source=self._name,
            metadata=metadata or {"internal": internal_str},
        )
        await self.publish_message(
            GroupChatMessage(message=message),
            topic_id=DefaultTopicId(type=self._output_topic_type),
        )
        await self._output_message_queue.put(message)

    async def _publish_group_chat_message(
        self,
        content: str,
        cancellation_token: CancellationToken,
        internal: bool = False,
        metadata: Optional[Dict[str, str]] = None,
    ) -> None:
        """Helper function to publish a group chat message."""
        internal_str = "yes" if internal else "no"
        message = TextMessage(
            content=content,
            source=self._name,
            metadata=metadata or {"internal": internal_str},
        )
        await self.publish_message(
            GroupChatMessage(message=message),
            topic_id=DefaultTopicId(type=self._output_topic_type),
        )
        await self._output_message_queue.put(message)
        await self.publish_message(
            GroupChatAgentResponse(agent_response=Response(chat_message=message)),
            topic_id=DefaultTopicId(type=self._group_topic_type),
            cancellation_token=cancellation_token,
        )

    async def _request_next_speaker(
        self, next_speaker: str, cancellation_token: CancellationToken
    ) -> None:
        """Helper function to request the next speaker."""
        if next_speaker == "no_action_agent":
            await self._orchestrate_step(cancellation_token)
            return

        next_speaker_topic_type = self._participant_name_to_topic_type[next_speaker]
        await self.publish_message(
            GroupChatRequestPublish(),
            topic_id=DefaultTopicId(type=next_speaker_topic_type),
            cancellation_token=cancellation_token,
        )

    async def _get_json_response(
        self,
        messages: List[LLMMessage],
        validate_json: Callable[[Dict[str, Any]], bool],
        cancellation_token: CancellationToken,
    ) -> Dict[str, Any] | None:
        """Get a JSON response from the model client.
        Args:
            messages (List[LLMMessage]): The messages to send to the model client.
            validate_json (callable): A function to validate the JSON response. The function should return True if the JSON response is valid, otherwise False.
            cancellation_token (CancellationToken): A token to cancel the request if needed.
        """
        retries = 0
        exception_message = ""
        # ğŸ”§ å¢å¼ºç½‘ç»œè¿æ¥ç¨³å®šæ€§
        network_retry_count = 0
        max_network_retries = 3
        
        try:
            while retries < self._config.max_json_retries:
                # Re-initialize model context to meet token limit quota
                await self._model_context.clear()
                for msg in messages:
                    await self._model_context.add_message(msg)
                if exception_message != "":
                    await self._model_context.add_message(
                        UserMessage(content=exception_message, source=self._name)
                    )
                token_limited_messages = await self._model_context.get_messages()

                # ğŸ”§ å¢å¼ºç½‘ç»œè¿æ¥é‡è¯•æœºåˆ¶
                try:
                    response = await self._model_client.create(
                        token_limited_messages,
                        json_output=True
                        if self._model_client.model_info["json_output"]
                        else False,
                        cancellation_token=cancellation_token,
                    )
                except Exception as network_error:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯
                    import openai
                    import httpx
                    
                    error_message = str(network_error)
                    is_network_error = (
                        isinstance(network_error, (openai.APIConnectionError, httpx.ConnectError)) or
                        "Connection error" in error_message or
                        "All connection attempts failed" in error_message or
                        "ConnectTimeout" in error_message or
                        "ReadTimeout" in error_message
                    )
                    
                    if is_network_error and network_retry_count < max_network_retries:
                        network_retry_count += 1
                        await self._log_message_agentchat(
                            f"ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ­£åœ¨é‡è¯•... ({network_retry_count}/{max_network_retries})", 
                            internal=False
                        )
                        
                        # é€’å¢ç­‰å¾…æ—¶é—´
                        import asyncio
                        wait_time = min(5 * network_retry_count, 30)  # 5ç§’ã€10ç§’ã€30ç§’
                        await asyncio.sleep(wait_time)
                        
                        # é‡è¯•å½“å‰è¯·æ±‚
                        continue
                    else:
                        # ç½‘ç»œé‡è¯•æ¬¡æ•°ç”¨å®Œæˆ–éç½‘ç»œé”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise network_error
                
                # é‡ç½®ç½‘ç»œé‡è¯•è®¡æ•°å™¨ï¼ŒæˆåŠŸè¿æ¥å
                network_retry_count = 0
                
                assert isinstance(response.content, str)
                try:
                    json_response = json.loads(response.content)
                    # Use the validate_json function to check the response
                    if validate_json(json_response):
                        return json_response
                    else:
                        exception_message = "Validation failed for JSON response, retrying. You must return a valid JSON object parsed from the response."
                        await self._log_message(
                            f"Validation failed for JSON response, retrying ({retries}/{self._config.max_json_retries})"
                        )
                except json.JSONDecodeError as e:
                    json_response = extract_json_from_string(response.content)
                    if json_response is not None:
                        if validate_json(json_response):
                            return json_response
                        else:
                            exception_message = "Validation failed for JSON response, retrying. You must return a valid JSON object parsed from the response."
                    else:
                        exception_message = f"Failed to parse JSON response, retrying. You must return a valid JSON object parsed from the response. Error: {e}"
                    await self._log_message(
                        f"Failed to parse JSON response, retrying ({retries}/{self._config.max_json_retries})"
                    )
                retries += 1
            await self._log_message_agentchat(
                "Failed to get a valid JSON response after multiple retries",
                internal=False,
            )
            raise ValueError(
                "Failed to get a valid JSON response after multiple retries"
            )
        except Exception as e:
            # ğŸ”§ å¢å¼ºç½‘ç»œé”™è¯¯å¤„ç†ï¼Œä½†ä¿ç•™ç°æœ‰é€»è¾‘
            import openai
            import httpx
            
            error_message = str(e)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯
            if (isinstance(e, (openai.APIConnectionError, httpx.ConnectError)) or
                "Connection error" in error_message or
                "All connection attempts failed" in error_message):
                
                await self._log_message_agentchat(
                    f"ğŸŒ ç½‘ç»œè¿æ¥æŒç»­å¤±è´¥ï¼Œä½†ä»»åŠ¡å°†ç»§ç»­ä»¥ç®€åŒ–æ¨¡å¼è¿è¡Œã€‚é”™è¯¯è¯¦æƒ…: {e}", 
                    internal=False
                )
                
                # ğŸ”§ æ”¹è¿›ï¼šä¸ç›´æ¥æŠ›å‡ºé”™è¯¯ï¼Œè€Œæ˜¯å°è¯•é™çº§å¤„ç†
                try:
                    # å°è¯•ç®€åŒ–çš„éJSONå“åº”æ¨¡å¼
                    trace_logger.info("ğŸ”„ å°è¯•ç®€åŒ–çš„éJSONæ¨¡å¼...")
                    
                    # ä½¿ç”¨æ›´ç®€å•çš„æ¶ˆæ¯
                    simple_message = UserMessage(
                        content="ç”±äºç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·ç”¨ç®€åŒ–çš„æ–¹å¼ç»§ç»­å¤„ç†ä»»åŠ¡ã€‚", 
                        source=self._name
                    )
                    
                    # å°è¯•éJSONå“åº”
                    response = await self._model_client.create(
                        [simple_message],
                        json_output=False,
                        cancellation_token=cancellation_token,
                    )
                    
                    # å¦‚æœæˆåŠŸï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„è§„åˆ’å“åº”
                    if response and response.content:
                        await self._log_message_agentchat(
                            "âœ… ç½‘ç»œè¿æ¥å·²æ¢å¤ï¼Œç»§ç»­æ‰§è¡Œä»»åŠ¡", internal=False
                        )
                        
                        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„è§„åˆ’ç»“æ„ï¼Œè®©ä»»åŠ¡ç»§ç»­
                        return {
                            "next_agent": "web_surfer",
                            "instruction": "ç»§ç»­æ‰§è¡Œä»»åŠ¡ï¼Œè®¿é—®teche720.comæŸ¥çœ‹å…¨æ™¯ç›¸æœºå‚è€ƒèµ„æ–™",
                            "reasoning": "ç½‘ç»œè¿æ¥æ¢å¤åç»§ç»­åŸå®šè®¡åˆ’"
                        }
                    
                except Exception as retry_error:
                    await self._log_message_agentchat(
                        f"âŒ ç½‘ç»œè¿æ¥å®Œå…¨å¤±è´¥ï¼Œä»»åŠ¡å°†æš‚åœ: {retry_error}", 
                        internal=False
                    )
                    # åªæœ‰åœ¨å®Œå…¨å¤±è´¥æ—¶æ‰æŠ›å‡ºé”™è¯¯
                    raise
            else:
                await self._log_message_agentchat(
                    f"Error in Orchestrator: {e}", internal=False
                )
            raise

    @rpc
    async def handle_start(self, message: GroupChatStart, ctx: MessageContext) -> None:  # type: ignore
        """Handle the start of a group chat by selecting a speaker to start the conversation."""
        # Check if the conversation has already terminated.
        if (
            self._termination_condition is not None
            and self._termination_condition.terminated
        ):
            early_stop_message = StopMessage(
                content="The group chat has already terminated.", source=self._name
            )
            await self._signal_termination(early_stop_message)

            # Stop the group chat.
            return
        assert message is not None and message.messages is not None

        # send message to all agents with initial user message
        await self.publish_message(
            GroupChatStart(messages=message.messages),
            topic_id=DefaultTopicId(type=self._group_topic_type),
            cancellation_token=ctx.cancellation_token,
        )

        # handle agent response
        for m in message.messages:
            self._state.message_history.append(m)
        await self._orchestrate_step(ctx.cancellation_token)

    async def pause(self) -> None:
        """Pause the group chat manager."""
        self._state.is_paused = True

    async def resume(self) -> None:
        """Resume the group chat manager."""
        self._state.is_paused = False

    @event
    async def handle_agent_response(  # type: ignore
        self, message: GroupChatAgentResponse, ctx: MessageContext
    ) -> None:
        delta: List[BaseChatMessage] = []
        if message.agent_response.inner_messages is not None:
            for inner_message in message.agent_response.inner_messages:
                delta.append(inner_message)  # type: ignore
        self._state.message_history.append(message.agent_response.chat_message)
        delta.append(message.agent_response.chat_message)

        if self._termination_condition is not None:
            stop_message = await self._termination_condition(delta)
            if stop_message is not None:
                await self._prepare_final_answer(
                    reason="Termination Condition Met.",
                    cancellation_token=ctx.cancellation_token,
                    force_stop=True,
                )
                await self._signal_termination(stop_message)
                # Stop the group chat and reset the termination conditions and turn count.
                await self._termination_condition.reset()
                return
        await self._orchestrate_step(ctx.cancellation_token)

    async def _orchestrate_step(self, cancellation_token: CancellationToken) -> None:
        """Orchestrate the next step of the conversation."""
        if self._state.is_paused:
            # let user speak next if paused
            await self._request_next_speaker(self._user_agent_topic, cancellation_token)
            return

        if self._state.in_planning_mode:
            await self._orchestrate_step_planning(cancellation_token)
        else:
            await self._orchestrate_step_execution(cancellation_token)

    async def do_bing_search(self, query: str) -> str | None:
        try:
            # log the bing search request
            await self._log_message_agentchat(
                "Searching online for information...",
                metadata={"internal": "no", "type": "progress_message"},
            )
            bing_search_results = await get_bing_search_results(
                query,
                max_pages=3,
                max_tokens_per_page=5000,
                timeout_seconds=35,
            )
            if bing_search_results.combined_content != "":
                bing_results_progress = f"Reading through {len(bing_search_results.page_contents)} web pages..."
                await self._log_message_agentchat(
                    bing_results_progress,
                    metadata={"internal": "no", "type": "progress_message"},
                )
                return bing_search_results.combined_content
            return None
        except Exception as e:
            trace_logger.exception(f"Error in doing bing search: {e}")
            return None

    async def _get_websurfer_page_info(self) -> None:
        """Get the page information from the web surfer agent."""
        try:
            if self._web_agent_topic in self._participant_names:
                web_surfer_container = (
                    await self._runtime.try_get_underlying_agent_instance(
                        AgentId(
                            type=self._participant_name_to_topic_type[
                                self._web_agent_topic
                            ],
                            key=self.id.key,
                        )
                    )
                )
                if (
                    web_surfer_container._agent is not None  # type: ignore
                ):
                    web_surfer = web_surfer_container._agent  # type: ignore
                    page_title: str | None = None
                    page_url: str | None = None
                    (page_title, page_url) = await web_surfer.get_page_title_url()  # type: ignore
                    assert page_title is not None
                    assert page_url is not None

                    num_tabs, tabs_information_str = await web_surfer.get_tabs_info()  # type: ignore
                    tabs_information_str = f"There are {num_tabs} tabs open. The tabs are as follows:\n{tabs_information_str}"

                    message = MultiModalMessage(
                        content=[tabs_information_str],
                        source="web_surfer",
                    )
                    if "about:blank" not in page_url:
                        page_description: str | None = None
                        screenshot: bytes | None = None
                        metadata_hash: str | None = None
                        (
                            page_description,  # type: ignore
                            screenshot,  # type: ignore
                            metadata_hash,  # type: ignore
                        ) = await web_surfer.describe_current_page()  # type: ignore
                        assert isinstance(screenshot, bytes)
                        assert isinstance(page_description, str)
                        assert isinstance(metadata_hash, str)
                        if metadata_hash != self._last_browser_metadata_hash:
                            page_description = (
                                "A description of the current page: " + page_description
                            )
                            self._last_browser_metadata_hash: str = metadata_hash

                            message.content.append(page_description)
                            message.content.append(
                                AGImage.from_pil(PIL.Image.open(io.BytesIO(screenshot)))
                            )
                    self._state.message_history.append(message)
        except Exception as e:
            trace_logger.exception(f"Error in getting web surfer screenshot: {e}")
            pass

    async def _handle_relevant_plan_from_memory(
        self,
        context: Optional[List[LLMMessage]] = None,
    ) -> Optional[Any]:
        """
        Handles retrieval of relevant plans from memory for 'reuse' or 'hint' modes.
        Returns:
            For 'reuse', returns the most relevant plan (or None).
            For 'hint', appends a relevant plan as a UserMessage to the context if found.
        """
        if not self._memory_controller:
            return None
        try:
            mode = self._config.retrieve_relevant_plans
            task = self._state.task
            source = self._name
            trace_logger.info(
                f"retrieving relevant plan from memory for mode: {mode} ..."
            )
            memos = await self._memory_controller.retrieve_relevant_memos(task=task)
            trace_logger.info(f"{len(memos)} relevant plan(s) retrieved from memory")
            if len(memos) > 0:
                most_relevant_plan = memos[0].insight
                if mode == "reuse":
                    return most_relevant_plan
                elif mode == "hint" and context is not None:
                    context.append(
                        UserMessage(
                            content="Relevant plan:\n " + most_relevant_plan,
                            source=source,
                        )
                    )
        except Exception as e:
            trace_logger.error(f"Error retrieving plans from memory: {e}")
        return None

    async def _orchestrate_step_planning(
        self, cancellation_token: CancellationToken
    ) -> None:
        # Planning stage
        plan_response: Dict[str, Any] | None = None
        last_user_message = self._state.message_history[-1]
        assert last_user_message.source in [self._user_agent_topic, "user"]
        message_content: str = ""
        assert isinstance(last_user_message, TextMessage | MultiModalMessage)

        if isinstance(last_user_message.content, list):
            # iterate over the list and get the first item that is a string
            for item in last_user_message.content:
                if isinstance(item, str):
                    message_content = item
                    break
        else:
            message_content = last_user_message.content
        last_user_message = HumanInputFormat.from_str(message_content)

        # Is this our first time planning?
        if self._state.task == "" and self._state.plan_str == "":
            self._state.task = "TASK: " + last_user_message.content

            # TCM reuse plan
            from_memory = False
            if (
                not self._config.plan
                and self._config.retrieve_relevant_plans == "reuse"
            ):
                most_relevant_plan = await self._handle_relevant_plan_from_memory()
                if most_relevant_plan is not None:
                    self._config.plan = Plan.from_list_of_dicts_or_str(
                        most_relevant_plan
                    )
                    from_memory = True
            # Do we already have a plan to follow and planning mode is disabled?
            if self._config.plan is not None:
                self._state.plan = self._config.plan
                self._state.plan_str = str(self._config.plan)
                self._state.message_history.append(
                    TextMessage(
                        content="Initial plan from user:\n " + str(self._config.plan),
                        source="user",
                    )
                )
                plan_response = {
                    "task": self._state.plan.task,
                    "steps": [step.model_dump() for step in self._state.plan.steps],
                    "needs_plan": True,
                    "response": "",
                    "plan_summary": self._state.plan.task,
                    "from_memory": from_memory,
                }

                await self._log_message_agentchat(
                    dict_to_str(plan_response),
                    metadata={"internal": "no", "type": "plan_message"},
                )

                if not self._config.cooperative_planning:
                    self._state.in_planning_mode = False
                    await self._orchestrate_first_step(cancellation_token)
                    return
                else:
                    await self._request_next_speaker(
                        self._user_agent_topic, cancellation_token
                    )
                    return
            # Did the user provide a plan?
            user_plan = last_user_message.plan
            if user_plan is not None:
                self._state.plan = user_plan
                self._state.plan_str = str(user_plan)
                if last_user_message.accepted or is_accepted_str(
                    last_user_message.content
                ):
                    self._state.in_planning_mode = False
                    await self._orchestrate_first_step(cancellation_token)
                    return

            # assume the task is the last user message
            context = self._thread_to_context()
            # if bing search is enabled, do a bing search to help with planning
            if self._config.do_bing_search:
                bing_search_results = await self.do_bing_search(
                    last_user_message.content
                )
                if bing_search_results is not None:
                    context.append(
                        UserMessage(
                            content=bing_search_results,
                            source="web_surfer",
                        )
                    )

            if self._config.retrieve_relevant_plans == "hint":
                await self._handle_relevant_plan_from_memory(context=context)

            # create a first plan
            context.append(
                UserMessage(
                    content=self._get_task_ledger_plan_prompt(self._team_description),
                    source=self._name,
                )
            )
            plan_response = await self._get_json_response(
                context, self._validate_plan_json, cancellation_token
            )
            if self._state.is_paused:
                # let user speak next if paused
                await self._request_next_speaker(
                    self._user_agent_topic, cancellation_token
                )
                return
            assert plan_response is not None
            self._state.plan = Plan.from_list_of_dicts_or_str(plan_response["steps"])
            self._state.plan_str = str(self._state.plan)
            # add plan_response to the message thread
            self._state.message_history.append(
                TextMessage(
                    content=json.dumps(plan_response, indent=4), source=self._name
                )
            )
        else:
            # what did the user say?
            # Check if user accepted the plan
            if last_user_message.accepted or is_accepted_str(last_user_message.content):
                user_plan = last_user_message.plan
                if user_plan is not None:
                    self._state.plan = user_plan
                    self._state.plan_str = str(user_plan)
                # switch to execution mode
                self._state.in_planning_mode = False
                await self._orchestrate_first_step(cancellation_token)
                return
            # user did not accept the plan yet
            else:
                # update the plan
                user_plan = last_user_message.plan
                if user_plan is not None:
                    self._state.plan = user_plan
                    self._state.plan_str = str(user_plan)

                context = self._thread_to_context()

                # if bing search is enabled, do a bing search to help with planning
                if self._config.do_bing_search:
                    bing_search_results = await self.do_bing_search(
                        last_user_message.content
                    )
                    if bing_search_results is not None:
                        context.append(
                            UserMessage(
                                content=bing_search_results,
                                source="web_surfer",
                            )
                        )
                if self._config.retrieve_relevant_plans == "hint":
                    await self._handle_relevant_plan_from_memory(context=context)
                context.append(
                    UserMessage(
                        content=self._get_task_ledger_plan_prompt(
                            self._team_description
                        ),
                        source=self._name,
                    )
                )
                plan_response = await self._get_json_response(
                    context, self._validate_plan_json, cancellation_token
                )
                if self._state.is_paused:
                    # let user speak next if paused
                    await self._request_next_speaker(
                        self._user_agent_topic, cancellation_token
                    )
                    return
                assert plan_response is not None
                self._state.plan = Plan.from_list_of_dicts_or_str(
                    plan_response["steps"]
                )
                self._state.plan_str = str(self._state.plan)
                if not self._config.no_overwrite_of_task:
                    self._state.task = plan_response["task"]
                # add plan_response to the message thread
                self._state.message_history.append(
                    TextMessage(
                        content=json.dumps(plan_response, indent=4), source=self._name
                    )
                )

        assert plan_response is not None
        # if we don't need to plan, just send the message
        if self._config.cooperative_planning:
            if not plan_response["needs_plan"]:
                # send the response plan_message["response"] to the group
                await self._publish_group_chat_message(
                    plan_response["response"], cancellation_token
                )
                await self._request_next_speaker(
                    self._user_agent_topic, cancellation_token
                )
                return
            else:
                await self._publish_group_chat_message(
                    dict_to_str(plan_response),
                    cancellation_token,
                    metadata={"internal": "no", "type": "plan_message"},
                )
                await self._request_next_speaker(
                    self._user_agent_topic, cancellation_token
                )
                return
        else:
            await self._publish_group_chat_message(
                dict_to_str(plan_response),
                metadata={"internal": "no", "type": "plan_message"},
                cancellation_token=cancellation_token,
            )
            self._state.in_planning_mode = False
            await self._orchestrate_first_step(cancellation_token)

    async def _orchestrate_first_step(
        self, cancellation_token: CancellationToken
    ) -> None:
        # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–æ¶ˆæ¯è¿‡æ»¤é€»è¾‘ï¼Œé¿å…åˆ é™¤é‡è¦é…ç½®ä¿¡æ¯
        # åªä¿ç•™ç”¨æˆ·æ¶ˆæ¯å’Œæœ€è¿‘çš„é‡è¦ç³»ç»Ÿæ¶ˆæ¯ï¼Œé¿å…è¿‡åº¦æ¸…ç†
        
        # ä¿ç•™ç”¨æˆ·æ¶ˆæ¯
        user_messages = [
            m for m in self._state.message_history
            if m.source in ["user", self._user_agent_topic]
        ]
        
        # ä¿ç•™æœ€è¿‘çš„è®¡åˆ’æ¶ˆæ¯
        recent_plan_messages = [
            m for m in self._state.message_history[-3:]  # åªæ£€æŸ¥æœ€è¿‘3æ¡æ¶ˆæ¯
            if hasattr(m, 'metadata') and m.metadata and 
               m.metadata.get('type') == 'plan_message'
        ]
        
        # ğŸ”§ é¿å…è¿‡åº¦æ¸…ç†ï¼šå¦‚æœæ¶ˆæ¯æ•°é‡åˆç†ï¼Œåˆ™ä¸è¿›è¡Œè¿‡æ»¤
        if len(self._state.message_history) <= 10:
            trace_logger.info(f"ğŸ”§ æ¶ˆæ¯å†å²æ•°é‡åˆç†({len(self._state.message_history)}æ¡)ï¼Œè·³è¿‡æ¸…ç†")
        else:
            # åªåœ¨æ¶ˆæ¯è¿‡å¤šæ—¶æ‰è¿›è¡Œæ¸…ç†
            filtered_messages: List[BaseChatMessage | BaseAgentEvent] = []
            for msg in self._state.message_history:
                if (msg in user_messages or 
                    msg in recent_plan_messages or 
                    msg.source == self._name):
                    filtered_messages.append(msg)
            
            # ç¡®ä¿è‡³å°‘ä¿ç•™æœ€è¿‘çš„5æ¡æ¶ˆæ¯
            if len(filtered_messages) < 5:
                filtered_messages = self._state.message_history[-5:]
            
            self._state.message_history = filtered_messages
            trace_logger.info(f"ğŸ”§ æ¸…ç†æ¶ˆæ¯å†å²ï¼šä»åŸæ¥çš„æ¶ˆæ¯æ•°é‡å‡å°‘åˆ° {len(filtered_messages)} æ¡")

        ledger_message = TextMessage(
            content=self._get_task_ledger_full_prompt(
                self._state.task, self._team_description, self._state.plan_str
            ),
            source=self._name,
        )
        # add the ledger message to the message thread internally
        self._state.message_history.append(ledger_message)
        await self._log_message_agentchat(ledger_message.content, internal=True)

        # check if the plan is empty, complete, or we have reached the max turns
        if (
            (not isinstance(self._state.plan, Plan) or len(self._state.plan) == 0)
            or (self._state.current_step_idx >= len(self._state.plan))
            or (
                self._config.max_turns is not None
                and self._state.n_rounds > self._config.max_turns
            )
        ):
            await self._prepare_final_answer("Max rounds reached.", cancellation_token)
            return
            
        # ğŸ”§ CRITICAL FIX: ç¡®ä¿ç¬¬ä¸€æ­¥ä»æ­£ç¡®çš„Agentå¼€å§‹ï¼Œè€Œä¸æ˜¯å‡è®¾å·²å®Œæˆ
        current_step = self._state.plan[self._state.current_step_idx]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ¥è‡ªæœŸæœ›Agentçš„å“åº”
        expected_agent = current_step.agent_name
        has_agent_response = any(
            msg.source == expected_agent for msg in self._state.message_history[-5:]
        )
        
        if not has_agent_response:
            # ç¬¬ä¸€æ­¥å°šæœªæ‰§è¡Œï¼Œç›´æ¥å¯åŠ¨å¯¹åº”çš„Agent
            trace_logger.info(f"ğŸš€ ç¬¬ä¸€æ­¥å°šæœªå¼€å§‹ï¼Œç›´æ¥å¯åŠ¨ {expected_agent}")
            
            step_instruction = self._generate_step_instruction(current_step, self._state.current_step_idx)
            
            message_to_send = TextMessage(
                content=step_instruction, source=self._name, metadata={"internal": "yes"}
            )
            self._state.message_history.append(message_to_send)
            
            await self._publish_group_chat_message(
                message_to_send.content, cancellation_token, internal=True
            )
            
            # è®°å½•æ­¥éª¤å¯åŠ¨
            json_step_execution = {
                "title": current_step.title,
                "index": self._state.current_step_idx,
                "details": current_step.details,
                "agent_name": expected_agent,
                "instruction": step_instruction,
                "progress_summary": f"å¯åŠ¨æ­¥éª¤ {self._state.current_step_idx + 1}: {current_step.title}",
                "plan_length": len(self._state.plan),
            }
            await self._log_message_agentchat(
                json.dumps(json_step_execution),
                metadata={"internal": "no", "type": "step_execution"},
            )
            
            # è¯·æ±‚Agentæ‰§è¡Œ
            await self._request_next_speaker(expected_agent, cancellation_token)
            return
        
        # å¦‚æœæœ‰Agentå“åº”ï¼Œåˆ™ç»§ç»­æ­£å¸¸çš„progress ledgeræµç¨‹
        self._state.n_rounds += 1
        context = self._thread_to_context()
        # Creat the progress ledger

        progress_ledger_prompt = self._get_progress_ledger_prompt(
            self._state.task,
            self._state.plan_str,
            self._state.current_step_idx,
            self._team_description,
            self._agent_execution_names,
        )
        context.append(UserMessage(content=progress_ledger_prompt, source=self._name))

        progress_ledger = await self._get_json_response(
            context, self._validate_ledger_json, cancellation_token
        )
        if self._state.is_paused:
            # let user speak next if paused
            await self._request_next_speaker(self._user_agent_topic, cancellation_token)
            return
        assert progress_ledger is not None

        await self._log_message_agentchat(dict_to_str(progress_ledger), internal=True)

        # Broadcast the next step
        new_instruction = self.get_agent_instruction(
            progress_ledger["instruction_or_question"]["answer"],
            progress_ledger["instruction_or_question"]["agent_name"],
        )

        message_to_send = TextMessage(
            content=new_instruction, source=self._name, metadata={"internal": "yes"}
        )
        self._state.message_history.append(message_to_send)  # My copy

        await self._publish_group_chat_message(
            message_to_send.content, cancellation_token, internal=True
        )
        json_step_execution = {
            "title": self._state.plan[self._state.current_step_idx].title,
            "index": self._state.current_step_idx,
            "details": self._state.plan[self._state.current_step_idx].details,
            "agent_name": progress_ledger["instruction_or_question"]["agent_name"],
            "instruction": progress_ledger["instruction_or_question"]["answer"],
            "progress_summary": progress_ledger["progress_summary"],
            "plan_length": len(self._state.plan),
        }
        await self._log_message_agentchat(
            json.dumps(json_step_execution),
            metadata={"internal": "no", "type": "step_execution"},
        )
        # Request that the step be completed
        valid_next_speaker: bool = False
        next_speaker = progress_ledger["instruction_or_question"]["agent_name"]
        
        # ğŸ”§ å¤„ç†ç©ºagent_nameï¼šä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†åˆ†é…é€»è¾‘
        if not next_speaker or next_speaker.strip() == "":
            instruction_content = progress_ledger["instruction_or_question"]["answer"]
            step_title = self._state.plan[self._state.current_step_idx].title
            
            next_speaker = self._assign_agent_for_task(instruction_content, step_title)
            
            trace_logger.info(f"ğŸ”§ è‡ªåŠ¨åˆ†é…ç©ºagent_name -> {next_speaker} (æ­¥éª¤: {step_title[:30]}, æŒ‡ä»¤: {instruction_content[:30]})")
        
        for participant_name in self._agent_execution_names:
            if participant_name == next_speaker:
                await self._request_next_speaker(next_speaker, cancellation_token)
                valid_next_speaker = True
                break
        if not valid_next_speaker:
            raise ValueError(
                f"Invalid next speaker: {next_speaker} from the ledger, participants are: {self._agent_execution_names}"
            )

    async def _orchestrate_step_execution(
        self, cancellation_token: CancellationToken
    ) -> None:
        # Execution stage
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦å¯åŠ¨æ–°æ­¥éª¤ï¼ˆé¿å…é€’å½’è°ƒç”¨é—®é¢˜ï¼‰
        if hasattr(self._state, '_should_start_next_step'):
            delattr(self._state, '_should_start_next_step')
            await self._initiate_next_step_execution(cancellation_token)
            return
        
        # Check if we reached the maximum number of rounds
        assert self._state.plan is not None
        if self._state.current_step_idx >= len(self._state.plan) or (
            self._config.max_turns is not None
            and self._state.n_rounds > self._config.max_turns
        ):
            await self._prepare_final_answer("Max rounds reached.", cancellation_token)
            return

        # ğŸ”§ å¢å¼ºçš„æ­¥éª¤çŠ¶æ€ç®¡ç†é€»è¾‘ - åŒ…å«è¾¹ç•Œæ£€æŸ¥
        current_step_idx = self._state.current_step_idx
        
        # åˆå§‹åŒ–å½“å‰æ­¥éª¤çŠ¶æ€
        self._init_step_status(current_step_idx)
        
        # ğŸ”§ æ£€æŸ¥ä»»åŠ¡è¾¹ç•Œæ˜¯å¦è¢«è§¦åŠ
        boundary_violations = self._check_task_boundaries(current_step_idx)
        if boundary_violations["should_force_complete"]:
            # ğŸ”§ è¾¾åˆ°è¾¹ç•Œé™åˆ¶ï¼Œå¼ºåˆ¶å®Œæˆå½“å‰æ­¥éª¤
            completion_reason: List[str] = []
            if boundary_violations["max_actions_exceeded"]:
                completion_reason.append("æ“ä½œæ¬¡æ•°è¶…é™")
            if boundary_violations["time_limit_exceeded"]:
                completion_reason.append("æ—¶é—´è¶…é™")
            if boundary_violations["repetition_detected"]:
                completion_reason.append("æ£€æµ‹åˆ°é‡å¤æ“ä½œ")
            
            reason = "ã€".join(completion_reason)
            evidence = f"ğŸ”„ å› {reason}å¼ºåˆ¶å®Œæˆæ­¥éª¤"
            
            # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿æ­¥éª¤æ²¡æœ‰è¢«é‡å¤æ ‡è®°ä¸ºå®Œæˆ
            if self._state.step_execution_status.get(current_step_idx) == "completed":
                trace_logger.warning(f"âš ï¸ æ­¥éª¤ {current_step_idx + 1} å·²ç»å®Œæˆï¼Œè·³è¿‡è¾¹ç•Œå¼ºåˆ¶å®Œæˆ")
                return
                
            self._mark_step_completed(current_step_idx, evidence, "boundary")
            
            # ğŸ”§ æ›´æ–°ä¸Šä¸‹æ–‡å¹¶ç»§ç»­ä¸‹ä¸€æ­¥
            self._update_global_context("system", current_step_idx, evidence)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ­¥éª¤é€’å¢çš„åŸå­æ€§
            old_step_idx = self._state.current_step_idx
            self._state.current_step_idx += 1
            
            trace_logger.info(f"â° æ­¥éª¤ {current_step_idx + 1} å› è¾¹ç•Œé™åˆ¶å¼ºåˆ¶å®Œæˆ: {reason}")
            trace_logger.info(f"ğŸš€ è¾¹ç•Œå¼ºåˆ¶æ­¥éª¤é€’å¢: {old_step_idx + 1} â†’ {self._state.current_step_idx + 1}")
            
            if self._state.current_step_idx >= len(self._state.plan):
                await self._prepare_final_answer("All steps completed with boundary limits", cancellation_token)
                return
            else:
                await self._orchestrate_step_execution(cancellation_token)
                return
        
        # å¦‚æœæ­¥éª¤è¿˜æ²¡å¼€å§‹ï¼Œæ ‡è®°ä¸ºè¿›è¡Œä¸­
        if self._state.step_execution_status[current_step_idx] == "not_started":
            self._mark_step_in_progress(current_step_idx)
        
        # ğŸ”§ ä¿®å¤æ­¥éª¤è·³è·ƒé—®é¢˜ï¼šåªæœ‰å½“å‰æ­¥éª¤çš„agentå“åº”æ‰èƒ½æ ‡è®°å½“å‰æ­¥éª¤å®Œæˆ
        current_step_completed = False
        if self._state.message_history:
            last_message = self._state.message_history[-1]
            if hasattr(last_message, 'source') and last_message.source != self._name:
                # è·å–å½“å‰æ­¥éª¤åº”è¯¥æ‰§è¡Œçš„agent
                expected_agent = self._state.plan[current_step_idx].agent_name
                actual_agent = getattr(last_message, 'source', None) if last_message else None
                agent_response = ""
                if last_message:
                    from autogen_agentchat.messages import MultiModalMessage, TextMessage
                    if isinstance(last_message, MultiModalMessage):
                        # ğŸ”§ å¢å¼ºçš„MultiModalMessageå¤„ç†
                        text_parts: List[str] = []
                        for part in last_message.content:
                            if isinstance(part, str):
                                text_parts.append(part)
                        agent_response = " ".join(text_parts)
                        
                        # ğŸ”§ å¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œç”ŸæˆåŸºäºè¡Œä¸ºçš„æè¿°
                        if not agent_response.strip():
                            agent_response = f"WebSurfer executed actions and accessed te720.com website"
                        
                        trace_logger.info(f"ğŸ” MultiModalMessageæ–‡æœ¬æå–: {len(text_parts)}ä¸ªæ–‡æœ¬ç‰‡æ®µ, æ€»é•¿åº¦: {len(agent_response)}")
                    elif isinstance(last_message, TextMessage):
                        agent_response = last_message.content
                    else:
                        agent_response = str(getattr(last_message, 'content', ''))
                # åªç”¨æ–‡æœ¬æ¨è¿›æµç¨‹
                if actual_agent == expected_agent:
                    self._state.current_step_agent_response_count += 1
                    action_key = f"step_{current_step_idx}_actions"
                    if action_key not in self._state.repetition_count:
                        self._state.repetition_count[action_key] = 0
                    if self._detect_repetitive_response(agent_response):
                        self._state.repetition_count[action_key] += 1
                    step_completion_result = self._is_step_truly_complete(current_step_idx, agent_response)
                    trace_logger.info(f"ğŸ” æ­¥éª¤ {current_step_idx + 1} å®Œæˆæ£€æŸ¥: {step_completion_result}, å“åº”å‰100å­—ç¬¦: {agent_response[:100]}")
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆæ£€æŸ¥WebSurferè‡ªåŠ¨å®Œæˆä¿¡å·
                    if not step_completion_result and actual_agent == "web_surfer":
                        # é¦–å…ˆæ£€æŸ¥æˆ‘ä»¬ä¿®å¤ä¸­æ·»åŠ çš„è‡ªåŠ¨å®Œæˆä¿¡å·
                        if self._check_websurfer_auto_completion_signals(agent_response):
                            trace_logger.info(f"ğŸ¯ WebSurferè‡ªåŠ¨å®Œæˆä¿¡å·æ£€æµ‹æˆåŠŸ")
                            step_completion_result = True
                    
                    # ğŸ”§ é‡è¦ä¿®å¤ï¼šå¢å¼ºWebSurferæ­¥éª¤å®Œæˆæ£€æŸ¥
                    if not step_completion_result and actual_agent == "web_surfer":
                        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šæ£€æŸ¥WebSurferé”™è¯¯æ¢å¤åœºæ™¯
                        websurfer_error_recovery_check = False
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«æˆåŠŸæ“ä½œçš„é”™è¯¯æ¢å¤
                        if "encountered an error" in agent_response.lower() or "screenshot" in agent_response.lower():
                            success_indicators = ["te720.com", "äº§å“", "product", "clicked", "è®¿é—®", "successfully accessed"]
                            has_successful_actions = any(indicator in agent_response.lower() for indicator in success_indicators)
                            
                            if has_successful_actions:
                                trace_logger.info(f"ğŸ”„ WebSurferé”™è¯¯æ¢å¤å®Œæˆæ£€æŸ¥ - åŒ…å«æˆåŠŸæ“ä½œè¯æ®")
                                websurfer_error_recovery_check = True
                        
                        # æ£€æŸ¥WebSurferæ˜¯å¦å› ä¸ºæ²¡æœ‰è°ƒç”¨stop_actionè€Œè¢«è¯¯åˆ¤ä¸ºæœªå®Œæˆ
                        if self._state.current_step_agent_response_count >= 2:  # å¦‚æœå·²ç»æœ‰2æ¬¡å“åº”
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å®è´¨å†…å®¹
                            if any(indicator in agent_response.lower() for indicator in ["te720", "360", "camera", "product", "hovered", "accessed"]):
                                trace_logger.info(f"ğŸ”§ WebSurferå¼ºåˆ¶å®Œæˆæ£€æŸ¥ - å·²æ‰§è¡Œ{self._state.current_step_agent_response_count}æ¬¡å“åº”ä¸”åŒ…å«å®è´¨å†…å®¹")
                                websurfer_error_recovery_check = True
                        
                        # ğŸ”§ å…³é”®ï¼šåº”ç”¨WebSurferç‰¹æ®Šå®Œæˆæ£€æŸ¥
                        if websurfer_error_recovery_check:
                            step_completion_result = True
                    
                    if step_completion_result:
                        # ğŸ”§ é˜²æŠ¤ï¼šç¡®ä¿æ­¥éª¤æ²¡æœ‰è¢«é‡å¤æ ‡è®°ä¸ºå®Œæˆ
                        if self._state.step_execution_status.get(current_step_idx) == "completed":
                            trace_logger.warning(f"âš ï¸ æ­¥éª¤ {current_step_idx + 1} å·²ç»å®Œæˆï¼Œè·³è¿‡é‡å¤å¤„ç†")
                            return
                        
                        # åªç”¨last_messageåšä¸Šä¸‹æ–‡æ›´æ–°ï¼Œå›¾ç‰‡å¯¹è±¡ä¸å¤–ä¼ 
                        if actual_agent:
                            self._update_global_context(actual_agent, current_step_idx, last_message)
                        self._mark_step_completed(current_step_idx, agent_response[:200], "normal")
                        
                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ­¥éª¤é€’å¢çš„åŸå­æ€§
                        old_step_idx = self._state.current_step_idx
                        self._state.current_step_idx += 1
                        self._state.current_step_agent_response_count = 0
                        self._state.last_agent_task_completion_signal = ""
                        
                        trace_logger.info(f"ğŸš€ æ­¥éª¤é€’å¢: {old_step_idx + 1} â†’ {self._state.current_step_idx + 1}")
                        
                        # å¯åŠ¨ä¸‹ä¸€æ­¥ï¼Œé¿å…é€’å½’
                        await self._initiate_next_step_execution(cancellation_token)
                        return
                else:
                    trace_logger.warning(f"âš ï¸ æ­¥éª¤ {current_step_idx + 1} æœŸæœ› {expected_agent} å“åº”ï¼Œä½†æ”¶åˆ° {actual_agent} å“åº”ï¼Œå¿½ç•¥æ­¤å“åº”")

        # å¦‚æœæ­¥éª¤å·²å®Œæˆï¼Œç›´æ¥è·³è¿‡å‰©ä½™é€»è¾‘
        if current_step_completed:
            return

        self._state.n_rounds += 1
        context = self._thread_to_context()
        
        # Update the progress ledger
        progress_ledger_prompt = self._get_progress_ledger_prompt(
            self._state.task,
            self._state.plan_str,
            self._state.current_step_idx,
            self._team_description,
            self._agent_execution_names,
        )

        context.append(UserMessage(content=progress_ledger_prompt, source=self._name))

        progress_ledger = await self._get_json_response(
            context, self._validate_ledger_json, cancellation_token
        )
        if self._state.is_paused:
            await self._request_next_speaker(self._user_agent_topic, cancellation_token)
            return
        assert progress_ledger is not None
        
        # log the progress ledger
        await self._log_message_agentchat(dict_to_str(progress_ledger), internal=True)

        # Check for replans
        need_to_replan = progress_ledger["need_to_replan"]["answer"]
        replan_reason = progress_ledger["need_to_replan"]["reason"]

        if need_to_replan and self._config.allow_for_replans:
            # Replan
            if self._config.max_replans is None:
                await self._replan(replan_reason, cancellation_token)
            elif self._state.n_replans < self._config.max_replans:
                self._state.n_replans += 1
                await self._replan(replan_reason, cancellation_token)
                return
            else:
                await self._prepare_final_answer(
                    f"We need to replan but max replan attempts reached: {replan_reason}.",
                    cancellation_token,
                )
                return
        elif need_to_replan:
            await self._prepare_final_answer(
                f"The current plan failed to complete the task, we need a new plan to continue. {replan_reason}",
                cancellation_token,
            )
            return
        
        # ğŸ”§ FIXED: ç§»é™¤é‡å¤çš„æ­¥éª¤ç´¢å¼•é€’å¢ - æ­¥éª¤æ¨è¿›ä»…åœ¨Agentå“åº”å¤„ç†ä¸­è¿›è¡Œ(line 1912)
        is_step_complete = progress_ledger["is_current_step_complete"]["answer"]
        if is_step_complete:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ­¥éª¤çœŸæ­£å®Œæˆ
            if self._state.step_execution_status.get(current_step_idx) == "completed":
                # âœ… æ­¥éª¤å·²åœ¨Agentå“åº”å¤„ç†ä¸­å®Œæˆï¼Œæ— éœ€é‡å¤é€’å¢
                trace_logger.info(f"âœ… æ­¥éª¤ {current_step_idx + 1} å·²é€šè¿‡Agentå“åº”ç¡®è®¤å®Œæˆ")
            else:
                # ğŸ”§ Progress Ledgerè®¤ä¸ºå®Œæˆä½†Agentå“åº”æœªç¡®è®¤ - ç»§ç»­ç­‰å¾…æ­£ç¡®å®Œæˆä¿¡å·
                self._state.current_step_agent_response_count += 1
                
                if self._state.current_step_agent_response_count >= 5:
                    # è®°å½•å¾ªç¯æ£€æµ‹ä½†ä¸å¼ºåˆ¶é€’å¢ - è®©Agentå“åº”å¤„ç†æœºåˆ¶å¤„ç†
                    trace_logger.warning(f"âš ï¸ æ­¥éª¤ {current_step_idx + 1} å¾ªç¯è¶…è¿‡5æ¬¡ï¼Œéœ€è¦Agentå‘é€æ­£ç¡®å®Œæˆä¿¡å·")
                    self._state.repetition_count[f"step_{current_step_idx}_force_warning"] = True
                    
                    # æ·»åŠ è­¦å‘Šæ¶ˆæ¯ä½†ä¸é€’å¢æ­¥éª¤ç´¢å¼•
                    from autogen_agentchat.messages import TextMessage
                    force_progress_msg = TextMessage(
                        content="ç³»ç»Ÿæ£€æµ‹åˆ°æ­¥éª¤å¯èƒ½å¾ªç¯ï¼Œè¯·Agentæ˜ç¡®å‘é€å®Œæˆä¿¡å·ã€‚",
                        source=self._name,
                        metadata={"internal": "no", "type": "system_warning"}
                    )
                    self._state.message_history.append(force_progress_msg)
                    await self._log_message_agentchat(force_progress_msg.content, internal=False)
                else:
                    trace_logger.info(f"âš ï¸ Progress Ledger è®¤ä¸ºæ­¥éª¤å®Œæˆï¼Œä½† Agent Response æœªç¡®è®¤ï¼Œç»§ç»­å½“å‰æ­¥éª¤ (é‡è¯• {self._state.current_step_agent_response_count}/5)")
        else:
            # é‡ç½®è®¡æ•°å™¨
            self._state.current_step_agent_response_count = 0

        if progress_ledger["progress_summary"] != "":
            self._state.information_collected += (
                "\n" + progress_ledger["progress_summary"]
            )
        
        # Check for plan completion
        if self._state.current_step_idx >= len(self._state.plan):
            await self._prepare_final_answer(
                "Plan completed.",
                cancellation_token,
            )
            return

        # Broadcast the next step
        new_instruction = self.get_agent_instruction(
            progress_ledger["instruction_or_question"]["answer"],
            progress_ledger["instruction_or_question"]["agent_name"],
        )
        from autogen_agentchat.messages import TextMessage
        message_to_send = TextMessage(
            content=new_instruction, source=self._name, metadata={"internal": "yes"}
        )
        self._state.message_history.append(message_to_send)  # My copy

        await self._publish_group_chat_message(
            message_to_send.content, cancellation_token, internal=True
        )
        json_step_execution = {
            "title": self._state.plan[self._state.current_step_idx].title,
            "index": self._state.current_step_idx,
            "details": self._state.plan[self._state.current_step_idx].details,
            "agent_name": progress_ledger["instruction_or_question"]["agent_name"],
            "instruction": progress_ledger["instruction_or_question"]["answer"],
            "progress_summary": progress_ledger["progress_summary"],
            "plan_length": len(self._state.plan),
        }
        await self._log_message_agentchat(
            json.dumps(json_step_execution),
            metadata={"internal": "no", "type": "step_execution"},
        )

        # Request that the step be completed
        valid_next_speaker: bool = False
        next_speaker = progress_ledger["instruction_or_question"]["agent_name"]
        
        # ğŸ”§ å¤„ç†ç©ºagent_nameï¼šä½¿ç”¨ç»Ÿä¸€çš„ä»£ç†åˆ†é…é€»è¾‘
        if not next_speaker or next_speaker.strip() == "":
            instruction_content = progress_ledger["instruction_or_question"]["answer"]
            step_title = self._state.plan[self._state.current_step_idx].title
            
            next_speaker = self._assign_agent_for_task(instruction_content, step_title)
            
            trace_logger.info(f"ğŸ”§ è‡ªåŠ¨åˆ†é…ç©ºagent_name -> {next_speaker} (æ­¥éª¤: {step_title[:30]}, æŒ‡ä»¤: {instruction_content[:30]})")
        
        for participant_name in self._agent_execution_names:
            if participant_name == next_speaker:
                await self._request_next_speaker(next_speaker, cancellation_token)
                valid_next_speaker = True
                break
        if not valid_next_speaker:
            raise ValueError(
                f"Invalid next speaker: {next_speaker} from the ledger, participants are: {self._agent_execution_names}"
            )

    async def _replan(self, reason: str, cancellation_token: CancellationToken) -> None:
        # Let's create a new plan
        self._state.in_planning_mode = True
        await self._log_message_agentchat(
            f"We need to create a new plan. {reason}",
            metadata={"internal": "no", "type": "replanning"},
        )
        context = self._thread_to_context()

        # Store completed steps
        completed_steps = (
            list(self._state.plan.steps[: self._state.current_step_idx])
            if self._state.plan
            else []
        )
        completed_plan_str = "\n".join(
            [f"COMPLETED STEP {i+1}: {step}" for i, step in enumerate(completed_steps)]
        )

        # Add completed steps info to replan prompt
        replan_prompt = self._get_task_ledger_replan_plan_prompt(
            self._state.task,
            self._team_description,
            f"Completed steps so far:\n{completed_plan_str}\n\nPrevious plan:\n{self._state.plan_str}",
        )
        context.append(
            UserMessage(
                content=replan_prompt,
                source=self._name,
            )
        )
        plan_response = await self._get_json_response(
            context, self._validate_plan_json, cancellation_token
        )
        assert plan_response is not None

        # Create new plan by combining completed steps with new steps
        new_plan = Plan.from_list_of_dicts_or_str(plan_response["steps"])
        if new_plan is not None:
            combined_steps = completed_steps + list(new_plan.steps)
            self._state.plan = Plan(task=self._state.task, steps=combined_steps)
            self._state.plan_str = str(self._state.plan)
        else:
            # If new plan is None, keep the completed steps
            self._state.plan = Plan(task=self._state.task, steps=completed_steps)
            self._state.plan_str = str(self._state.plan)

        # Update task if in planning mode
        if not self._config.no_overwrite_of_task:
            self._state.task = plan_response["task"]

        plan_response["plan_summary"] = "Replanning: " + plan_response["plan_summary"]
        # Log the plan response in the same format as planning mode
        await self._publish_group_chat_message(
            dict_to_str(plan_response),
            cancellation_token=cancellation_token,
            metadata={"internal": "no", "type": "plan_message"},
        )
        # next speaker is user
        if self._config.cooperative_planning:
            await self._request_next_speaker(self._user_agent_topic, cancellation_token)
        else:
            self._state.in_planning_mode = False
            await self._orchestrate_first_step(cancellation_token)

    async def _initiate_next_step_execution(self, cancellation_token: CancellationToken):
        """ğŸ”§ å¯åŠ¨ä¸‹ä¸€æ­¥æ‰§è¡Œï¼Œé¿å…é€’å½’è°ƒç”¨é—®é¢˜"""
        
        current_step_idx = self._state.current_step_idx
        
        if self._state.plan is None or current_step_idx >= len(self._state.plan):
            await self._prepare_final_answer("All steps completed", cancellation_token)
            return
            
        current_step = self._state.plan[current_step_idx]
        
        # ç”Ÿæˆæ–°æ­¥éª¤çš„æ˜ç¡®æŒ‡ä»¤
        step_instruction = self._generate_step_instruction(current_step, current_step_idx)
        
        # åˆ›å»ºæ¶ˆæ¯å¯åŠ¨ä¸‹ä¸€æ­¥
        message_to_send = TextMessage(
            content=step_instruction,
            source=self._name,
            metadata={"internal": "yes", "step_index": str(current_step_idx)}
        )
        
        # å‘å¸ƒæ¶ˆæ¯
        await self._publish_group_chat_message(
            message_to_send.content, cancellation_token, internal=True
        )
        
        # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        self._state.message_history.append(message_to_send)
        
        trace_logger.info(f"ğŸš€ å¯åŠ¨æ­¥éª¤ {current_step_idx + 1}: {current_step.title}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¯·æ±‚å¯¹åº”çš„Agentæ‰§è¡Œæ–°æ­¥éª¤
        next_agent = current_step.agent_name
        if not next_agent or next_agent.strip() == "":
            # å¦‚æœagent_nameä¸ºç©ºï¼Œä½¿ç”¨æ™ºèƒ½åˆ†é…é€»è¾‘
            next_agent = self._assign_agent_for_task(step_instruction, current_step.title)
            trace_logger.info(f"ğŸ”§ è‡ªåŠ¨åˆ†é…ç©ºagent_name -> {next_agent}")
        
        # è¯·æ±‚Agentæ‰§è¡Œ
        await self._request_next_speaker(next_agent, cancellation_token)

    def _generate_step_instruction(self, step: Any, step_idx: int) -> str:
        """ä¸ºæ­¥éª¤ç”Ÿæˆå…·ä½“çš„æ‰§è¡ŒæŒ‡ä»¤ï¼Œè‡ªåŠ¨è¡¥å……å›¾ç‰‡æ’å›¾è¯´æ˜"""
        step_title_lower = step.title.lower()
        step_agent = step.agent_name.lower()
        context = self._state.global_context
        image_hint = ""
        if context.get("image_generated") and context.get("generated_image_base64"):
            image_hint = "\n\nè¯·åœ¨æ–‡æ¡£/é¡µé¢å¼€å¤´æ’å…¥å¦‚ä¸‹å›¾ç‰‡ï¼ˆbase64ï¼‰ï¼š![](data:image/png;base64,{})\n".format(context["generated_image_base64"][:60] + '...')
        
        # ğŸ”§ FIXED: ä¸ºWebSurferç¬¬ä¸€æ­¥æ·»åŠ ä¸“é—¨é€»è¾‘
        if step_agent == "web_surfer" and ("gather" in step_title_lower or "te720" in step_title_lower):
            return f"""
Step {step_idx + 1}: {step.title}

{step.details}

ğŸ”§ **WEBSURFER TASK GUIDANCE**:
- Visit te720.com website to gather information about 360 panoramic cameras
- Look for product images and technical specifications
- Focus on cameras with 4-lens configurations
- Extract key product features and descriptions
- Use stop_action with completion signal when sufficient information is collected

COMPLETION SIGNALS:
- âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ: Successfully gathered product information
- âš ï¸ å½“å‰æ­¥éª¤å› é”™è¯¯å®Œæˆ: Website inaccessible but provided alternative information

AUTONOMOUS MODE: Navigate freely without approval requests for research purposes.
"""
        
        elif 'html' in step_title_lower or 'format' in step_title_lower:
            return f"""
Step {step_idx + 1}: {step.title}

{step.details}

Instruction for {step.agent_name}: Convert the Markdown product introduction to HTML format with proper styling and layout.{image_hint}

ğŸ”§ **HTML CONVERSION GUIDANCE**:
- Read the generated Markdown file: 360_panoramic_camera_intro.md
- Create an HTML version with professional styling
- Include proper CSS for layout and visual appeal
- Ensure the generated image is properly embedded
- Use embedded CSS for self-contained HTML
- OUTPUT: Create .html file for PDF conversion

IMPORTANT: Include the generated panoramic camera image in the HTML document.
"""
        elif 'pdf' in step_title_lower:
            return f"""
Step {step_idx + 1}: {step.title}

{step.details}

Instruction for {step.agent_name}: Convert the HTML document to PDF format as the final deliverable.{image_hint}

ğŸ”§ **PDF CONVERSION GUIDANCE**:
- Read the HTML file created in the previous step
- Convert to PDF format using libraries like weasyprint or pdfkit
- Ensure proper page layout and formatting
- Include all images and styling
- Generate high-quality PDF output
- OUTPUT: Generate final PDF document named '360_panoramic_camera_product.pdf'

IMPORTANT: The final PDF should include both the generated image and the product information.
"""
        elif 'markdown' in step_title_lower or 'md' in step_title_lower:
            return f"""
Step {step_idx + 1}: {step.title}

{step.details}

Instruction for {step.agent_name}: è¯·ç»“åˆç½‘é¡µé‡‡é›†ä¿¡æ¯å’Œä¸Šä¸€è½®ç”Ÿæˆçš„å›¾ç‰‡ï¼Œå†™ä¸€ä»½äº§å“ä»‹ç»ï¼Œå›¾ç‰‡æ”¾åœ¨æ–‡æ¡£å¼€å¤´ã€‚{image_hint}

ğŸ”§ **MARKDOWN DOCUMENT GUIDANCE**:
- ç»“åˆ te720.com é‡‡é›†åˆ°çš„äº§å“ä¿¡æ¯
- åœ¨æ–‡æ¡£å¼€å¤´æ’å…¥ç”Ÿæˆçš„ç›¸æœºå›¾ç‰‡
- ç»“æ„åŒ–äº§å“ä»‹ç»ï¼Œçªå‡ºæŠ€æœ¯äº®ç‚¹
- OUTPUT: ç”Ÿæˆ .md æ–‡ä»¶ï¼Œä¾›åç»­ HTML/PDF è½¬æ¢
"""
        else:
            return f"""
Step {step_idx + 1}: {step.title}

{step.details}

Instruction for {step.agent_name}: {step.details}{image_hint}

ğŸ”§ **EXECUTION GUIDANCE**:
- Focus on the specific requirements of this step
- Provide clear completion signals when finished
- Ensure all outputs are properly saved and accessible
"""

    async def _prepare_final_answer(
        self,
        reason: str,
        cancellation_token: CancellationToken,
        final_answer: str | None = None,
        force_stop: bool = False,
    ) -> None:
        """Prepare the final answer for the task.

        Args:
            reason (str): The reason for preparing the final answer
            cancellation_token (CancellationToken): Token for cancellation
            final_answer (str, optional): Optional pre-computed final answer to use instead of computing one
            force_stop (bool): Whether to force stop the conversation after the final answer is computed
        """
        if final_answer is None:
            context = self._thread_to_context()
            # add replan reason
            context.append(UserMessage(content=reason, source=self._name))
            # Get the final answer
            final_answer_prompt = self._get_final_answer_prompt(self._state.task)
            progress_summary = f"Progress Summary:\n{self._state.information_collected}"
            context.append(
                UserMessage(
                    content=progress_summary + "\n\n" + final_answer_prompt,
                    source=self._name,
                )
            )

            # Re-initialize model context to meet token limit quota
            await self._model_context.clear()
            for msg in context:
                await self._model_context.add_message(msg)
            token_limited_context = await self._model_context.get_messages()

            response = await self._model_client.create(
                token_limited_context, cancellation_token=cancellation_token
            )
            assert isinstance(response.content, str)
            final_answer = response.content

        message = TextMessage(
            content=f"Final Answer: {final_answer}", source=self._name
        )

        self._state.message_history.append(message)  # My copy

        await self._publish_group_chat_message(
            message.content,
            cancellation_token,
            metadata={"internal": "no", "type": "final_answer"},
        )

        # reset internals except message history
        self._state.reset_for_followup()
        if not force_stop and self._config.allow_follow_up_input:
            await self._request_next_speaker(self._user_agent_topic, cancellation_token)
        else:
            # Signal termination
            await self._signal_termination(
                StopMessage(content=reason, source=self._name)
            )

        if self._termination_condition is not None:
            await self._termination_condition.reset()

    def _thread_to_context(
        self, messages: Optional[List[BaseChatMessage | BaseAgentEvent]] = None
    ) -> List[LLMMessage]:
        """Convert the message thread to a context for the model."""
        chat_messages: List[BaseChatMessage | BaseAgentEvent] = (
            messages if messages is not None else self._state.message_history
        )
        context_messages: List[LLMMessage] = []
        date_today = datetime.now().strftime("%d %B, %Y")
        if self._state.in_planning_mode:
            context_messages.append(
                SystemMessage(content=self._get_system_message_planning())
            )
        else:
            context_messages.append(
                SystemMessage(
                    content=ORCHESTRATOR_SYSTEM_MESSAGE_EXECUTION.format(
                        date_today=date_today
                    )
                )
            )
        if self._model_client.model_info["vision"]:
            context_messages.extend(
                thread_to_context(
                    messages=chat_messages, agent_name=self._name, is_multimodal=True
                )
            )
        else:
            context_messages.extend(
                thread_to_context(
                    messages=chat_messages, agent_name=self._name, is_multimodal=False
                )
            )
        return context_messages

    async def save_state(self) -> Mapping[str, Any]:
        """Save the current state of the orchestrator.

        Returns:
            Mapping[str, Any]: A dictionary containing all state attributes except is_paused.
        """
        # Get all state attributes except message_history and is_paused
        data = self._state.model_dump(exclude={"is_paused"})

        # Serialize message history separately to preserve type information
        data["message_history"] = [
            message.dump() for message in self._state.message_history
        ]

        # Serialize plan if it exists
        if self._state.plan is not None:
            data["plan"] = self._state.plan.model_dump()

        return data

    async def load_state(self, state: Mapping[str, Any]) -> None:
        """Load a previously saved state into the orchestrator.

        Args:
            state (Mapping[str, Any]): A dictionary containing the state attributes to load.
        """
        # Create new state with defaults
        new_state = OrchestratorState()

        # Load basic attributes
        for key, value in state.items():
            if key == "message_history":
                # Handle message history separately
                new_state.message_history = [
                    self._message_factory.create(message) for message in value
                ]
            elif key == "plan" and value is not None:
                # Reconstruct Plan object if it exists
                new_state.plan = Plan(**value)
            elif key != "is_paused" and hasattr(new_state, key):
                setattr(new_state, key, value)

        # Update the state
        self._state = new_state

    # ========================================
    # æ™ºèƒ½å¢å¼ºæ–¹æ³•
    # ========================================
    
    def _create_intelligent_instruction(self, step_info: dict, agent_name: str) -> str:
        """
        åˆ›å»ºæ™ºèƒ½æŒ‡ä»¤ - åŸºäºä¸Šä¸‹æ–‡å’Œæ‰§è¡Œå†å²
        """
        execution_state = self._analyze_execution_state(step_info["index"], agent_name)
        
        base_instruction = step_info.get("details", "")
        
        # æ·»åŠ æ™ºèƒ½ç­–ç•¥æŒ‡å¯¼
        strategy_guidance = self._generate_strategy_guidance(agent_name, execution_state)
        
        # æ·»åŠ é—®é¢˜è§£å†³æŒ‡å¯¼
        problem_solving = self._generate_problem_solving_guidance(execution_state)
        
        # æ·»åŠ æ™ºèƒ½å®ŒæˆæŒ‡å¯¼
        completion_guidance = self._generate_completion_guidance(agent_name, execution_state)
        
        # ç»„åˆæœ€ç»ˆæŒ‡ä»¤
        final_instruction = f"""{base_instruction}

{strategy_guidance}

{problem_solving}

{completion_guidance}"""
        
        return final_instruction.strip()
    
    def _generate_strategy_guidance(self, agent_name: str, state: dict) -> str:
        """ç”Ÿæˆç­–ç•¥æŒ‡å¯¼"""
        guidance = f"ğŸ§  **æ™ºèƒ½æ‰§è¡Œç­–ç•¥** (å°è¯• {state['attempts']+1}):\n"
        
        if agent_name == "web_surfer":
            if state["attempts"] == 0:
                guidance += "- æ ‡å‡†æµè§ˆï¼šç³»ç»Ÿæ€§è®¿é—®ä¸»è¦é¡µé¢æ”¶é›†ä¿¡æ¯\n"
            elif state["attempts"] < 3:
                guidance += "- æ•ˆç‡æµè§ˆï¼šä¸“æ³¨æ ¸å¿ƒä¿¡æ¯ï¼Œå‡å°‘å¯¼èˆªæ·±åº¦\n"
            else:
                guidance += "- å¿«é€Ÿå®Œæˆï¼šä½¿ç”¨å·²æœ‰ä¿¡æ¯ï¼Œé¿å…é‡å¤æ“ä½œ\n"
        
        elif agent_name == "image_generator":
            guidance += "- åŸºäºæ”¶é›†çš„äº§å“ä¿¡æ¯ç”Ÿæˆå‡†ç¡®çš„è§†è§‰è¡¨ç°\n"
            guidance += "- ç¡®ä¿å›¾åƒè´¨é‡å’Œä¸“ä¸šæ€§\n"
        
        elif agent_name == "coder_agent":
            guidance += "- æ™ºèƒ½æ–‡æ¡£å¤„ç†ï¼šç†è§£æ ¼å¼éœ€æ±‚å’Œè½¬æ¢è¦æ±‚\n"
            guidance += "- ç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§å’Œæ ¼å¼æ­£ç¡®æ€§\n"
        
        return guidance
    
    def _generate_problem_solving_guidance(self, state: dict) -> str:
        """ç”Ÿæˆé—®é¢˜è§£å†³æŒ‡å¯¼"""
        guidance = "ğŸ”§ **æ™ºèƒ½é—®é¢˜è§£å†³**:\n"
        guidance += "- é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼šè®°å½•å¹¶ä½¿ç”¨å¯ç”¨ä¿¡æ¯ç»§ç»­\n"
        guidance += "- èµ„æºä¸å¯ç”¨ï¼šç«‹å³é‡‡ç”¨æ›¿ä»£æ–¹æ¡ˆ\n"
        guidance += "- è¶…æ—¶æˆ–é”™è¯¯ï¼šä¼˜é›…é™çº§ï¼Œç¡®ä¿éƒ¨åˆ†å®Œæˆ\n"
        
        if state["attempts"] >= 3:
            guidance += "- å¤šæ¬¡å°è¯•ï¼šä¼˜å…ˆä½¿ç”¨ç´¯ç§¯ä¿¡æ¯ç›´æ¥å®Œæˆ\n"
        
        return guidance
    
    def _generate_completion_guidance(self, agent_name: str, state: dict) -> str:
        """ç”Ÿæˆå®ŒæˆæŒ‡å¯¼"""
        guidance = "âš¡ **æ™ºèƒ½å®Œæˆæœºåˆ¶**:\n"
        
        if agent_name == "web_surfer":
            guidance += "- å®Œæˆä¿¡å·ï¼šä½¿ç”¨ 'âœ… å½“å‰æ­¥éª¤å·²å®Œæˆ' æ˜ç¡®è¡¨ç¤ºå®Œæˆ\n"
            guidance += "- ä¿¡æ¯æ ‡å‡†ï¼šæ”¶é›†åŸºæœ¬äº§å“ä¿¡æ¯å³å¯å®Œæˆ\n"
            guidance += "- æ•ˆç‡ä¼˜å…ˆï¼šé¿å…è¿‡åº¦æµè§ˆï¼Œé‡ç‚¹ä¿¡æ¯ä¼˜å…ˆ\n"
        
        elif agent_name == "image_generator":
            guidance += "- å®Œæˆä¿¡å·ï¼šç”ŸæˆåæŠ¥å‘Š 'å›¾åƒç”Ÿæˆä»»åŠ¡å·²å®Œæˆ'\n"
            guidance += "- è´¨é‡ç¡®ä¿ï¼šç¡®ä¿å›¾åƒç¬¦åˆæè¿°è¦æ±‚\n"
        
        elif agent_name == "coder_agent":
            guidance += "- å®Œæˆä¿¡å·ï¼šå¤„ç†åæŠ¥å‘Š 'æ–‡æ¡£åˆ›å»ºä»»åŠ¡å·²å®Œæˆ'\n"
            guidance += "- æ–‡ä»¶ç¡®è®¤ï¼šç¡®ä¿è¾“å‡ºæ–‡ä»¶æ­£ç¡®å¯è®¿é—®\n"
        
        # æ·»åŠ è¾¹ç•Œä¿¡æ¯
        boundaries = state.get("boundaries", {})
        if boundaries:
            guidance += f"\nğŸ“‹ **æ‰§è¡Œå‚æ•°**:\n"
            guidance += f"- æœ€å¤§æ“ä½œï¼š{boundaries.get('max_actions', 5)}\n"
            guidance += f"- å½“å‰å°è¯•ï¼š{state['attempts'] + 1}\n"
        
        return guidance
